@ Stage 2
2022-08-18 19:48:26 | INFO | fairseq_cli.train | Namespace(activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='mbart_base', attention_dropout=0.0, batch_size=32, batch_size_valid=32, best_checkpoint_metric='bleu', bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, dstore_filename=None, dstore_size=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eval_bleu=True, eval_bleu_args='{"beam": 6}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, k=5, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, knn_lambda=0.5, knn_sim_metric=None, knn_temperature=10, label_smoothing=0.1, langs='java,python,en_XX', layernorm_embedding=True, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format='json', log_interval=100, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, move_dstore_to_mem=False, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, only_train_final_output=False, optimizer='adam', optimizer_overrides='{}', partially_finetune=False, patience=5, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, prepend_bos=False, probe=32, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='/home/cluster/jgu/scratch/ssr/cli/model/plbart_base.pt', save_dir='/home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_java_en_XX', save_interval=1, save_interval_updates=0, scoring='bleu', seed=42, sentence_avg=False, sentencepiece_model='/home/cluster/jgu/scratch/ssr/cli/sentencepiece/sentencepiece.bpe.model', shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='java', stop_time_hours=0, target_lang='en_XX', task='translation_from_pretrained_bart', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_gpu_to_search=False, use_knn_datastore=False, use_old_adam=False, user_dir='/home/cluster/jgu/scratch/ssr/cli', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=1000, weight_decay=0.0, zero_sharding='none')
2022-08-18 19:48:26 | INFO | fairseq.tasks.translation | [java] dictionary: 50001 types
2022-08-18 19:48:26 | INFO | fairseq.tasks.translation | [en_XX] dictionary: 50001 types
2022-08-18 19:48:27 | INFO | fairseq.data.data_utils | loaded 8714 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin/valid.java-en_XX.java
2022-08-18 19:48:27 | INFO | fairseq.data.data_utils | loaded 8714 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin/valid.java-en_XX.en_XX
2022-08-18 19:48:27 | INFO | fairseq.tasks.translation | /home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin valid java-en_XX 8714 examples
2022-08-18 19:48:30 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=50005, bias=False)
  )
  (classification_heads): ModuleDict()
)
2022-08-18 19:48:30 | INFO | fairseq_cli.train | task: translation_from_pretrained_bart (TranslationFromPretrainedBARTTask)
2022-08-18 19:48:30 | INFO | fairseq_cli.train | model: mbart_base (BARTModel)
2022-08-18 19:48:30 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2022-08-18 19:48:30 | INFO | fairseq_cli.train | num. model params: 139220736 (num. trained: 139220736)
2022-08-18 19:48:35 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-08-18 19:48:35 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-08-18 19:48:35 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-08-18 19:48:35 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM2-32GB                    
2022-08-18 19:48:35 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-08-18 19:48:35 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-08-18 19:48:35 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 32
2022-08-18 19:48:41 | INFO | fairseq.trainer | loaded checkpoint /home/cluster/jgu/scratch/ssr/cli/model/plbart_base.pt (epoch 11 @ 0 updates)
2022-08-18 19:48:41 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2022-08-18 19:48:42 | INFO | fairseq.trainer | loading train data for epoch 1
2022-08-18 19:48:42 | INFO | fairseq.data.data_utils | loaded 69708 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin/train.java-en_XX.java
2022-08-18 19:48:42 | INFO | fairseq.data.data_utils | loaded 69708 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin/train.java-en_XX.en_XX
2022-08-18 19:48:42 | INFO | fairseq.tasks.translation | /home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin train java-en_XX 69708 examples
2022-08-18 19:48:42 | INFO | fairseq.trainer | begin training epoch 1
2022-08-18 19:49:01 | INFO | train_inner | {"epoch": 1, "update": 0.046, "loss": "8.743", "nll_loss": "6.992", "ppl": "127.3", "wps": "3373.1", "ups": "5.33", "wpb": "630.2", "bsz": "32", "num_updates": "100", "lr": "5e-06", "gnorm": "33.935", "train_wall": "19", "wall": "26"}
2022-08-18 19:49:22 | INFO | train_inner | {"epoch": 1, "update": 0.092, "loss": "6.691", "nll_loss": "4.997", "ppl": "31.93", "wps": "3009.9", "ups": "4.84", "wpb": "621.5", "bsz": "32", "num_updates": "200", "lr": "1e-05", "gnorm": "6.904", "train_wall": "20", "wall": "47"}
2022-08-18 19:49:42 | INFO | train_inner | {"epoch": 1, "update": 0.138, "loss": "6.271", "nll_loss": "4.638", "ppl": "24.89", "wps": "3065.3", "ups": "4.94", "wpb": "620", "bsz": "32", "num_updates": "300", "lr": "1.5e-05", "gnorm": "6.141", "train_wall": "20", "wall": "67"}
2022-08-18 19:50:01 | INFO | train_inner | {"epoch": 1, "update": 0.184, "loss": "6.165", "nll_loss": "4.552", "ppl": "23.46", "wps": "3633.9", "ups": "5.32", "wpb": "682.9", "bsz": "32", "num_updates": "400", "lr": "2e-05", "gnorm": "5.566", "train_wall": "19", "wall": "86"}
2022-08-18 19:50:22 | INFO | train_inner | {"epoch": 1, "update": 0.229, "loss": "6.103", "nll_loss": "4.502", "ppl": "22.66", "wps": "3208", "ups": "4.75", "wpb": "674.7", "bsz": "32", "num_updates": "500", "lr": "2.5e-05", "gnorm": "4.777", "train_wall": "21", "wall": "107"}
2022-08-18 19:50:42 | INFO | train_inner | {"epoch": 1, "update": 0.275, "loss": "5.987", "nll_loss": "4.39", "ppl": "20.96", "wps": "3330.7", "ups": "4.98", "wpb": "668.4", "bsz": "32", "num_updates": "600", "lr": "3e-05", "gnorm": "4.705", "train_wall": "20", "wall": "127"}
2022-08-18 19:51:01 | INFO | train_inner | {"epoch": 1, "update": 0.321, "loss": "5.88", "nll_loss": "4.281", "ppl": "19.44", "wps": "3275.9", "ups": "5.15", "wpb": "636.1", "bsz": "31.8", "num_updates": "700", "lr": "3.5e-05", "gnorm": "4.879", "train_wall": "19", "wall": "146"}
2022-08-18 19:51:20 | INFO | train_inner | {"epoch": 1, "update": 0.367, "loss": "5.91", "nll_loss": "4.33", "ppl": "20.11", "wps": "3308.8", "ups": "5.4", "wpb": "612.8", "bsz": "32", "num_updates": "800", "lr": "4e-05", "gnorm": "4.513", "train_wall": "18", "wall": "165"}
2022-08-18 19:51:36 | INFO | train_inner | {"epoch": 1, "update": 0.413, "loss": "5.864", "nll_loss": "4.282", "ppl": "19.45", "wps": "4067.8", "ups": "6.01", "wpb": "677.2", "bsz": "32", "num_updates": "900", "lr": "4.5e-05", "gnorm": "4.291", "train_wall": "16", "wall": "182"}
2022-08-18 19:51:57 | INFO | train_inner | {"epoch": 1, "update": 0.459, "loss": "5.896", "nll_loss": "4.326", "ppl": "20.06", "wps": "3520.9", "ups": "4.96", "wpb": "709.4", "bsz": "32", "num_updates": "1000", "lr": "5e-05", "gnorm": "4.073", "train_wall": "20", "wall": "202"}
2022-08-18 19:52:17 | INFO | train_inner | {"epoch": 1, "update": 0.505, "loss": "5.804", "nll_loss": "4.232", "ppl": "18.79", "wps": "3001.2", "ups": "4.91", "wpb": "610.7", "bsz": "32", "num_updates": "1100", "lr": "4.9995e-05", "gnorm": "4.203", "train_wall": "20", "wall": "222"}
2022-08-18 19:52:37 | INFO | train_inner | {"epoch": 1, "update": 0.551, "loss": "5.824", "nll_loss": "4.259", "ppl": "19.15", "wps": "3064.5", "ups": "4.92", "wpb": "623.3", "bsz": "32", "num_updates": "1200", "lr": "4.999e-05", "gnorm": "4.107", "train_wall": "20", "wall": "242"}
2022-08-18 19:52:57 | INFO | train_inner | {"epoch": 1, "update": 0.597, "loss": "5.733", "nll_loss": "4.159", "ppl": "17.87", "wps": "3230.3", "ups": "5.19", "wpb": "622.2", "bsz": "32", "num_updates": "1300", "lr": "4.9985e-05", "gnorm": "4.2", "train_wall": "19", "wall": "262"}
2022-08-18 19:53:16 | INFO | train_inner | {"epoch": 1, "update": 0.642, "loss": "5.788", "nll_loss": "4.227", "ppl": "18.72", "wps": "3300.7", "ups": "5.13", "wpb": "643.6", "bsz": "32", "num_updates": "1400", "lr": "4.998e-05", "gnorm": "3.77", "train_wall": "19", "wall": "281"}
2022-08-18 19:53:36 | INFO | train_inner | {"epoch": 1, "update": 0.688, "loss": "5.808", "nll_loss": "4.25", "ppl": "19.03", "wps": "3557.6", "ups": "5.04", "wpb": "705.6", "bsz": "32", "num_updates": "1500", "lr": "4.9975e-05", "gnorm": "3.866", "train_wall": "20", "wall": "301"}
2022-08-18 19:53:57 | INFO | train_inner | {"epoch": 1, "update": 0.734, "loss": "5.781", "nll_loss": "4.221", "ppl": "18.65", "wps": "3252.4", "ups": "4.7", "wpb": "691.8", "bsz": "32", "num_updates": "1600", "lr": "4.997e-05", "gnorm": "3.895", "train_wall": "21", "wall": "322"}
2022-08-18 19:54:16 | INFO | train_inner | {"epoch": 1, "update": 0.78, "loss": "5.765", "nll_loss": "4.211", "ppl": "18.52", "wps": "3651.1", "ups": "5.36", "wpb": "681.6", "bsz": "32", "num_updates": "1700", "lr": "4.9965e-05", "gnorm": "3.718", "train_wall": "18", "wall": "341"}
2022-08-18 19:54:37 | INFO | train_inner | {"epoch": 1, "update": 0.826, "loss": "5.644", "nll_loss": "4.073", "ppl": "16.83", "wps": "3152.9", "ups": "4.77", "wpb": "661.4", "bsz": "32", "num_updates": "1800", "lr": "4.996e-05", "gnorm": "3.791", "train_wall": "21", "wall": "362"}
2022-08-18 19:54:56 | INFO | train_inner | {"epoch": 1, "update": 0.872, "loss": "5.711", "nll_loss": "4.151", "ppl": "17.76", "wps": "3794.9", "ups": "5.18", "wpb": "732.5", "bsz": "32", "num_updates": "1900", "lr": "4.9955e-05", "gnorm": "3.817", "train_wall": "19", "wall": "381"}
2022-08-18 19:55:16 | INFO | train_inner | {"epoch": 1, "update": 0.918, "loss": "5.605", "nll_loss": "4.036", "ppl": "16.41", "wps": "2954.5", "ups": "4.97", "wpb": "594.6", "bsz": "32", "num_updates": "2000", "lr": "4.99499e-05", "gnorm": "3.902", "train_wall": "20", "wall": "401"}
2022-08-18 19:55:34 | INFO | train_inner | {"epoch": 1, "update": 0.964, "loss": "5.594", "nll_loss": "4.03", "ppl": "16.34", "wps": "3396", "ups": "5.69", "wpb": "597.1", "bsz": "32", "num_updates": "2100", "lr": "4.99449e-05", "gnorm": "3.632", "train_wall": "17", "wall": "419"}
2022-08-18 19:55:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
/net/cephfs/scratch/jgu/ssr/fairseq/utils.py:342: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2022-08-18 19:57:39 | INFO | valid | {"epoch": 1, "valid_loss": "5.418", "valid_nll_loss": "3.756", "valid_ppl": "13.51", "valid_bleu": "6.11", "valid_wps": "1617.5", "valid_wpb": "652.8", "valid_bsz": "31.9", "valid_num_updates": "2179"}
2022-08-18 19:57:39 | INFO | fairseq_cli.train | begin save checkpoint
2022-08-18 19:57:45 | INFO | fairseq.checkpoint_utils | saved checkpoint /home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_java_en_XX/checkpoint_best.pt (epoch 1 @ 2179 updates, score 6.11) (writing took 6.154064528644085 seconds)
2022-08-18 19:57:45 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-08-18 19:57:45 | INFO | train | {"epoch": 1, "train_loss": "6.008", "train_nll_loss": "4.418", "train_ppl": "21.38", "train_wps": "2621.6", "train_ups": "4.01", "train_wpb": "653", "train_bsz": "32", "train_num_updates": "2179", "train_lr": "4.9941e-05", "train_gnorm": "5.763", "train_train_wall": "422", "train_wall": "550"}
2022-08-18 19:57:45 | INFO | fairseq.trainer | begin training epoch 2
/net/cephfs/scratch/jgu/ssr/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beams_buf = indices_buf // vocab_size
/net/cephfs/scratch/jgu/ssr/fairseq/sequence_generator.py:651: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  unfin_idx = idx // beam_size
2022-08-18 19:57:49 | INFO | train_inner | {"epoch": 2, "update": 1.01, "loss": "5.563", "nll_loss": "3.992", "ppl": "15.91", "wps": "487.9", "ups": "0.74", "wpb": "661.8", "bsz": "32", "num_updates": "2200", "lr": "4.99399e-05", "gnorm": "3.623", "train_wall": "18", "wall": "555"}
2022-08-18 19:58:11 | INFO | train_inner | {"epoch": 2, "update": 1.056, "loss": "5.368", "nll_loss": "3.762", "ppl": "13.56", "wps": "3000.3", "ups": "4.57", "wpb": "657.1", "bsz": "32", "num_updates": "2300", "lr": "4.99349e-05", "gnorm": "3.612", "train_wall": "22", "wall": "576"}
2022-08-18 19:58:32 | INFO | train_inner | {"epoch": 2, "update": 1.101, "loss": "5.412", "nll_loss": "3.815", "ppl": "14.07", "wps": "3558.3", "ups": "4.97", "wpb": "716.6", "bsz": "32", "num_updates": "2400", "lr": "4.99299e-05", "gnorm": "3.661", "train_wall": "20", "wall": "597"}
2022-08-18 19:58:52 | INFO | train_inner | {"epoch": 2, "update": 1.147, "loss": "5.248", "nll_loss": "3.627", "ppl": "12.36", "wps": "3328.1", "ups": "4.95", "wpb": "671.8", "bsz": "32", "num_updates": "2500", "lr": "4.99249e-05", "gnorm": "3.66", "train_wall": "20", "wall": "617"}
2022-08-18 19:59:11 | INFO | train_inner | {"epoch": 2, "update": 1.193, "loss": "5.253", "nll_loss": "3.636", "ppl": "12.43", "wps": "3279.6", "ups": "5.18", "wpb": "633.6", "bsz": "32", "num_updates": "2600", "lr": "4.99199e-05", "gnorm": "3.625", "train_wall": "19", "wall": "636"}
2022-08-18 19:59:29 | INFO | train_inner | {"epoch": 2, "update": 1.239, "loss": "5.251", "nll_loss": "3.635", "ppl": "12.42", "wps": "3684.4", "ups": "5.48", "wpb": "672.9", "bsz": "32", "num_updates": "2700", "lr": "4.99149e-05", "gnorm": "3.801", "train_wall": "18", "wall": "654"}
2022-08-18 19:59:48 | INFO | train_inner | {"epoch": 2, "update": 1.285, "loss": "5.303", "nll_loss": "3.699", "ppl": "12.99", "wps": "3637.9", "ups": "5.37", "wpb": "677.6", "bsz": "32", "num_updates": "2800", "lr": "4.99099e-05", "gnorm": "3.65", "train_wall": "18", "wall": "673"}
2022-08-18 20:00:08 | INFO | train_inner | {"epoch": 2, "update": 1.331, "loss": "5.263", "nll_loss": "3.656", "ppl": "12.61", "wps": "3427.1", "ups": "5.06", "wpb": "677.7", "bsz": "32", "num_updates": "2900", "lr": "4.99049e-05", "gnorm": "3.634", "train_wall": "20", "wall": "693"}
2022-08-18 20:00:26 | INFO | train_inner | {"epoch": 2, "update": 1.377, "loss": "5.193", "nll_loss": "3.572", "ppl": "11.9", "wps": "3662.2", "ups": "5.35", "wpb": "685", "bsz": "32", "num_updates": "3000", "lr": "4.98999e-05", "gnorm": "3.495", "train_wall": "19", "wall": "711"}
2022-08-18 20:00:47 | INFO | train_inner | {"epoch": 2, "update": 1.423, "loss": "5.317", "nll_loss": "3.718", "ppl": "13.16", "wps": "3417.8", "ups": "4.85", "wpb": "704.2", "bsz": "32", "num_updates": "3100", "lr": "4.98949e-05", "gnorm": "3.554", "train_wall": "20", "wall": "732"}
2022-08-18 20:01:06 | INFO | train_inner | {"epoch": 2, "update": 1.469, "loss": "5.303", "nll_loss": "3.701", "ppl": "13.01", "wps": "3683.3", "ups": "5.22", "wpb": "706.2", "bsz": "31.8", "num_updates": "3200", "lr": "4.98899e-05", "gnorm": "3.565", "train_wall": "19", "wall": "751"}
2022-08-18 20:01:24 | INFO | train_inner | {"epoch": 2, "update": 1.514, "loss": "5.118", "nll_loss": "3.496", "ppl": "11.28", "wps": "3292.8", "ups": "5.6", "wpb": "588", "bsz": "32", "num_updates": "3300", "lr": "4.98849e-05", "gnorm": "3.904", "train_wall": "18", "wall": "769"}
2022-08-18 20:01:43 | INFO | train_inner | {"epoch": 2, "update": 1.56, "loss": "5.279", "nll_loss": "3.681", "ppl": "12.83", "wps": "3275.9", "ups": "5.14", "wpb": "637.3", "bsz": "32", "num_updates": "3400", "lr": "4.98799e-05", "gnorm": "3.678", "train_wall": "19", "wall": "789"}
2022-08-18 20:02:04 | INFO | train_inner | {"epoch": 2, "update": 1.606, "loss": "5.283", "nll_loss": "3.685", "ppl": "12.86", "wps": "3311.9", "ups": "4.91", "wpb": "674", "bsz": "32", "num_updates": "3500", "lr": "4.98749e-05", "gnorm": "3.65", "train_wall": "20", "wall": "809"}
2022-08-18 20:02:24 | INFO | train_inner | {"epoch": 2, "update": 1.652, "loss": "5.301", "nll_loss": "3.71", "ppl": "13.08", "wps": "3203.4", "ups": "4.86", "wpb": "658.7", "bsz": "32", "num_updates": "3600", "lr": "4.98699e-05", "gnorm": "3.62", "train_wall": "20", "wall": "829"}
2022-08-18 20:02:43 | INFO | train_inner | {"epoch": 2, "update": 1.698, "loss": "5.253", "nll_loss": "3.654", "ppl": "12.58", "wps": "3324.1", "ups": "5.29", "wpb": "628.1", "bsz": "32", "num_updates": "3700", "lr": "4.98649e-05", "gnorm": "3.707", "train_wall": "19", "wall": "848"}
2022-08-18 20:03:02 | INFO | train_inner | {"epoch": 2, "update": 1.744, "loss": "5.23", "nll_loss": "3.634", "ppl": "12.42", "wps": "3486", "ups": "5.48", "wpb": "635.7", "bsz": "32", "num_updates": "3800", "lr": "4.98599e-05", "gnorm": "3.6", "train_wall": "18", "wall": "867"}
2022-08-18 20:03:23 | INFO | train_inner | {"epoch": 2, "update": 1.79, "loss": "5.149", "nll_loss": "3.536", "ppl": "11.6", "wps": "3051.2", "ups": "4.73", "wpb": "644.7", "bsz": "32", "num_updates": "3900", "lr": "4.98549e-05", "gnorm": "3.715", "train_wall": "21", "wall": "888"}
2022-08-18 20:03:41 | INFO | train_inner | {"epoch": 2, "update": 1.836, "loss": "5.159", "nll_loss": "3.55", "ppl": "11.71", "wps": "3347.5", "ups": "5.33", "wpb": "627.7", "bsz": "32", "num_updates": "4000", "lr": "4.98498e-05", "gnorm": "3.709", "train_wall": "19", "wall": "906"}
2022-08-18 20:04:01 | INFO | train_inner | {"epoch": 2, "update": 1.882, "loss": "5.064", "nll_loss": "3.445", "ppl": "10.89", "wps": "2922.7", "ups": "5.12", "wpb": "570.8", "bsz": "32", "num_updates": "4100", "lr": "4.98448e-05", "gnorm": "3.658", "train_wall": "19", "wall": "926"}
2022-08-18 20:04:19 | INFO | train_inner | {"epoch": 2, "update": 1.927, "loss": "5.078", "nll_loss": "3.463", "ppl": "11.03", "wps": "3101.3", "ups": "5.57", "wpb": "556.8", "bsz": "32", "num_updates": "4200", "lr": "4.98398e-05", "gnorm": "3.607", "train_wall": "18", "wall": "944"}
2022-08-18 20:04:39 | INFO | train_inner | {"epoch": 2, "update": 1.973, "loss": "5.193", "nll_loss": "3.589", "ppl": "12.04", "wps": "3461", "ups": "5.08", "wpb": "681.9", "bsz": "32", "num_updates": "4300", "lr": "4.98348e-05", "gnorm": "3.632", "train_wall": "20", "wall": "964"}
2022-08-18 20:04:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 20:06:50 | INFO | valid | {"epoch": 2, "valid_loss": "5.129", "valid_nll_loss": "3.464", "valid_ppl": "11.04", "valid_bleu": "9.59", "valid_wps": "1505.7", "valid_wpb": "652.8", "valid_bsz": "31.9", "valid_num_updates": "4358", "valid_best_bleu": "9.59"}
2022-08-18 20:06:50 | INFO | fairseq_cli.train | begin save checkpoint
2022-08-18 20:06:57 | INFO | fairseq.checkpoint_utils | saved checkpoint /home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_java_en_XX/checkpoint_best.pt (epoch 2 @ 4358 updates, score 9.59) (writing took 6.364395044744015 seconds)
2022-08-18 20:06:57 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-08-18 20:06:57 | INFO | train | {"epoch": 2, "train_loss": "5.241", "train_nll_loss": "3.634", "train_ppl": "12.42", "train_wps": "2579.8", "train_ups": "3.95", "train_wpb": "653", "train_bsz": "32", "train_num_updates": "4358", "train_lr": "4.98319e-05", "train_gnorm": "3.652", "train_train_wall": "422", "train_wall": "1102"}
2022-08-18 20:06:57 | INFO | fairseq.trainer | begin training epoch 3
2022-08-18 20:07:06 | INFO | train_inner | {"epoch": 3, "update": 2.019, "loss": "5.038", "nll_loss": "3.411", "ppl": "10.63", "wps": "446.5", "ups": "0.68", "wpb": "658", "bsz": "32", "num_updates": "4400", "lr": "4.98298e-05", "gnorm": "3.49", "train_wall": "21", "wall": "1111"}
2022-08-18 20:07:27 | INFO | train_inner | {"epoch": 3, "update": 2.065, "loss": "4.863", "nll_loss": "3.199", "ppl": "9.18", "wps": "3220.5", "ups": "4.7", "wpb": "684.9", "bsz": "32", "num_updates": "4500", "lr": "4.98248e-05", "gnorm": "3.551", "train_wall": "21", "wall": "1132"}
2022-08-18 20:07:48 | INFO | train_inner | {"epoch": 3, "update": 2.111, "loss": "4.865", "nll_loss": "3.206", "ppl": "9.22", "wps": "3252.1", "ups": "4.86", "wpb": "669.5", "bsz": "32", "num_updates": "4600", "lr": "4.98198e-05", "gnorm": "3.494", "train_wall": "20", "wall": "1153"}
2022-08-18 20:08:07 | INFO | train_inner | {"epoch": 3, "update": 2.157, "loss": "4.806", "nll_loss": "3.138", "ppl": "8.8", "wps": "3150.1", "ups": "5.11", "wpb": "616.4", "bsz": "32", "num_updates": "4700", "lr": "4.98148e-05", "gnorm": "3.571", "train_wall": "19", "wall": "1172"}
2022-08-18 20:08:25 | INFO | train_inner | {"epoch": 3, "update": 2.203, "loss": "4.602", "nll_loss": "2.913", "ppl": "7.53", "wps": "3353", "ups": "5.82", "wpb": "575.9", "bsz": "32", "num_updates": "4800", "lr": "4.98098e-05", "gnorm": "3.648", "train_wall": "17", "wall": "1190"}
2022-08-18 20:08:45 | INFO | train_inner | {"epoch": 3, "update": 2.249, "loss": "4.868", "nll_loss": "3.214", "ppl": "9.28", "wps": "3214.5", "ups": "4.81", "wpb": "668.2", "bsz": "32", "num_updates": "4900", "lr": "4.98048e-05", "gnorm": "3.614", "train_wall": "21", "wall": "1210"}
2022-08-18 20:09:06 | INFO | train_inner | {"epoch": 3, "update": 2.295, "loss": "4.93", "nll_loss": "3.286", "ppl": "9.76", "wps": "3509.4", "ups": "4.95", "wpb": "708.4", "bsz": "32", "num_updates": "5000", "lr": "4.97998e-05", "gnorm": "3.624", "train_wall": "20", "wall": "1231"}
2022-08-18 20:09:25 | INFO | train_inner | {"epoch": 3, "update": 2.341, "loss": "4.672", "nll_loss": "2.995", "ppl": "7.97", "wps": "3211.4", "ups": "5.17", "wpb": "620.9", "bsz": "32", "num_updates": "5100", "lr": "4.97948e-05", "gnorm": "3.611", "train_wall": "19", "wall": "1250"}
2022-08-18 20:09:45 | INFO | train_inner | {"epoch": 3, "update": 2.386, "loss": "4.755", "nll_loss": "3.087", "ppl": "8.5", "wps": "3256.5", "ups": "5.01", "wpb": "650", "bsz": "32", "num_updates": "5200", "lr": "4.97898e-05", "gnorm": "3.685", "train_wall": "20", "wall": "1270"}
2022-08-18 20:10:05 | INFO | train_inner | {"epoch": 3, "update": 2.432, "loss": "4.805", "nll_loss": "3.144", "ppl": "8.84", "wps": "3211.8", "ups": "4.85", "wpb": "662.1", "bsz": "32", "num_updates": "5300", "lr": "4.97848e-05", "gnorm": "3.682", "train_wall": "20", "wall": "1290"}
2022-08-18 20:10:26 | INFO | train_inner | {"epoch": 3, "update": 2.478, "loss": "4.836", "nll_loss": "3.183", "ppl": "9.08", "wps": "3112.5", "ups": "4.85", "wpb": "641.3", "bsz": "31.8", "num_updates": "5400", "lr": "4.97798e-05", "gnorm": "3.699", "train_wall": "20", "wall": "1311"}
2022-08-18 20:10:46 | INFO | train_inner | {"epoch": 3, "update": 2.524, "loss": "4.778", "nll_loss": "3.118", "ppl": "8.68", "wps": "3123.3", "ups": "5.11", "wpb": "611.6", "bsz": "32", "num_updates": "5500", "lr": "4.97748e-05", "gnorm": "3.719", "train_wall": "19", "wall": "1331"}
2022-08-18 20:11:06 | INFO | train_inner | {"epoch": 3, "update": 2.57, "loss": "4.848", "nll_loss": "3.195", "ppl": "9.16", "wps": "3453.7", "ups": "4.86", "wpb": "710.8", "bsz": "32", "num_updates": "5600", "lr": "4.97698e-05", "gnorm": "3.657", "train_wall": "20", "wall": "1351"}
2022-08-18 20:11:25 | INFO | train_inner | {"epoch": 3, "update": 2.616, "loss": "4.797", "nll_loss": "3.142", "ppl": "8.83", "wps": "3262.5", "ups": "5.23", "wpb": "623.9", "bsz": "32", "num_updates": "5700", "lr": "4.97648e-05", "gnorm": "3.753", "train_wall": "19", "wall": "1370"}
2022-08-18 20:11:43 | INFO | train_inner | {"epoch": 3, "update": 2.662, "loss": "4.761", "nll_loss": "3.103", "ppl": "8.59", "wps": "3364.5", "ups": "5.61", "wpb": "599.6", "bsz": "32", "num_updates": "5800", "lr": "4.97598e-05", "gnorm": "3.719", "train_wall": "18", "wall": "1388"}
2022-08-18 20:12:01 | INFO | train_inner | {"epoch": 3, "update": 2.708, "loss": "4.896", "nll_loss": "3.258", "ppl": "9.57", "wps": "3627", "ups": "5.46", "wpb": "664.1", "bsz": "32", "num_updates": "5900", "lr": "4.97548e-05", "gnorm": "3.684", "train_wall": "18", "wall": "1407"}
2022-08-18 20:12:20 | INFO | train_inner | {"epoch": 3, "update": 2.754, "loss": "4.755", "nll_loss": "3.097", "ppl": "8.56", "wps": "3489.5", "ups": "5.51", "wpb": "633.1", "bsz": "32", "num_updates": "6000", "lr": "4.97497e-05", "gnorm": "3.724", "train_wall": "18", "wall": "1425"}
2022-08-18 20:12:40 | INFO | train_inner | {"epoch": 3, "update": 2.799, "loss": "4.944", "nll_loss": "3.313", "ppl": "9.94", "wps": "3384.2", "ups": "4.88", "wpb": "693.2", "bsz": "32", "num_updates": "6100", "lr": "4.97447e-05", "gnorm": "3.657", "train_wall": "20", "wall": "1445"}
2022-08-18 20:12:59 | INFO | train_inner | {"epoch": 3, "update": 2.845, "loss": "4.694", "nll_loss": "3.032", "ppl": "8.18", "wps": "3303.9", "ups": "5.42", "wpb": "609.4", "bsz": "32", "num_updates": "6200", "lr": "4.97397e-05", "gnorm": "3.677", "train_wall": "18", "wall": "1464"}
2022-08-18 20:13:17 | INFO | train_inner | {"epoch": 3, "update": 2.891, "loss": "4.715", "nll_loss": "3.053", "ppl": "8.3", "wps": "3276", "ups": "5.32", "wpb": "615.3", "bsz": "32", "num_updates": "6300", "lr": "4.97347e-05", "gnorm": "3.81", "train_wall": "19", "wall": "1482"}
2022-08-18 20:13:37 | INFO | train_inner | {"epoch": 3, "update": 2.937, "loss": "4.82", "nll_loss": "3.172", "ppl": "9.01", "wps": "3440.4", "ups": "5.1", "wpb": "674.4", "bsz": "32", "num_updates": "6400", "lr": "4.97297e-05", "gnorm": "3.666", "train_wall": "19", "wall": "1502"}
2022-08-18 20:13:57 | INFO | train_inner | {"epoch": 3, "update": 2.983, "loss": "4.914", "nll_loss": "3.275", "ppl": "9.68", "wps": "3837", "ups": "5.06", "wpb": "758.7", "bsz": "32", "num_updates": "6500", "lr": "4.97247e-05", "gnorm": "3.6", "train_wall": "20", "wall": "1522"}
2022-08-18 20:14:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 20:16:15 | INFO | valid | {"epoch": 3, "valid_loss": "5.001", "valid_nll_loss": "3.299", "valid_ppl": "9.84", "valid_bleu": "13.88", "valid_wps": "1360.9", "valid_wpb": "652.8", "valid_bsz": "31.9", "valid_num_updates": "6537", "valid_best_bleu": "13.88"}
2022-08-18 20:16:15 | INFO | fairseq_cli.train | begin save checkpoint
2022-08-18 20:16:22 | INFO | fairseq.checkpoint_utils | saved checkpoint /home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_java_en_XX/checkpoint_best.pt (epoch 3 @ 6537 updates, score 13.88) (writing took 6.390352740883827 seconds)
2022-08-18 20:16:22 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-08-18 20:16:22 | INFO | train | {"epoch": 3, "train_loss": "4.814", "train_nll_loss": "3.158", "train_ppl": "8.93", "train_wps": "2517.9", "train_ups": "3.86", "train_wpb": "653", "train_bsz": "32", "train_num_updates": "6537", "train_lr": "4.97229e-05", "train_gnorm": "3.653", "train_train_wall": "422", "train_wall": "1667"}
2022-08-18 20:16:22 | INFO | fairseq.trainer | begin training epoch 4
2022-08-18 20:16:34 | INFO | train_inner | {"epoch": 4, "update": 3.029, "loss": "4.737", "nll_loss": "3.072", "ppl": "8.41", "wps": "450.2", "ups": "0.64", "wpb": "707.1", "bsz": "32", "num_updates": "6600", "lr": "4.97197e-05", "gnorm": "3.542", "train_wall": "18", "wall": "1679"}
2022-08-18 20:16:52 | INFO | train_inner | {"epoch": 4, "update": 3.075, "loss": "4.447", "nll_loss": "2.736", "ppl": "6.66", "wps": "3687.6", "ups": "5.53", "wpb": "667", "bsz": "32", "num_updates": "6700", "lr": "4.97147e-05", "gnorm": "3.582", "train_wall": "18", "wall": "1697"}
2022-08-18 20:17:12 | INFO | train_inner | {"epoch": 4, "update": 3.121, "loss": "4.38", "nll_loss": "2.653", "ppl": "6.29", "wps": "3357.9", "ups": "4.94", "wpb": "679.9", "bsz": "32", "num_updates": "6800", "lr": "4.97097e-05", "gnorm": "3.691", "train_wall": "20", "wall": "1717"}
2022-08-18 20:17:32 | INFO | train_inner | {"epoch": 4, "update": 3.167, "loss": "4.383", "nll_loss": "2.662", "ppl": "6.33", "wps": "3100", "ups": "5.06", "wpb": "613.1", "bsz": "32", "num_updates": "6900", "lr": "4.97047e-05", "gnorm": "3.74", "train_wall": "20", "wall": "1737"}
2022-08-18 20:17:53 | INFO | train_inner | {"epoch": 4, "update": 3.212, "loss": "4.498", "nll_loss": "2.792", "ppl": "6.92", "wps": "3209.1", "ups": "4.67", "wpb": "687.2", "bsz": "32", "num_updates": "7000", "lr": "4.96997e-05", "gnorm": "3.768", "train_wall": "21", "wall": "1758"}
2022-08-18 20:18:13 | INFO | train_inner | {"epoch": 4, "update": 3.258, "loss": "4.44", "nll_loss": "2.731", "ppl": "6.64", "wps": "3157.7", "ups": "5.06", "wpb": "623.8", "bsz": "32", "num_updates": "7100", "lr": "4.96947e-05", "gnorm": "3.742", "train_wall": "20", "wall": "1778"}
2022-08-18 20:18:32 | INFO | train_inner | {"epoch": 4, "update": 3.304, "loss": "4.449", "nll_loss": "2.742", "ppl": "6.69", "wps": "3504.6", "ups": "5.42", "wpb": "646.7", "bsz": "32", "num_updates": "7200", "lr": "4.96897e-05", "gnorm": "3.742", "train_wall": "18", "wall": "1797"}
2022-08-18 20:18:51 | INFO | train_inner | {"epoch": 4, "update": 3.35, "loss": "4.426", "nll_loss": "2.716", "ppl": "6.57", "wps": "3171.6", "ups": "5.01", "wpb": "633.1", "bsz": "32", "num_updates": "7300", "lr": "4.96847e-05", "gnorm": "3.776", "train_wall": "20", "wall": "1817"}
2022-08-18 20:19:11 | INFO | train_inner | {"epoch": 4, "update": 3.396, "loss": "4.351", "nll_loss": "2.631", "ppl": "6.2", "wps": "3224.5", "ups": "5.22", "wpb": "617.3", "bsz": "32", "num_updates": "7400", "lr": "4.96797e-05", "gnorm": "3.784", "train_wall": "19", "wall": "1836"}
2022-08-18 20:19:30 | INFO | train_inner | {"epoch": 4, "update": 3.442, "loss": "4.449", "nll_loss": "2.746", "ppl": "6.71", "wps": "3381.3", "ups": "5.28", "wpb": "640.9", "bsz": "32", "num_updates": "7500", "lr": "4.96747e-05", "gnorm": "3.849", "train_wall": "19", "wall": "1855"}
2022-08-18 20:19:49 | INFO | train_inner | {"epoch": 4, "update": 3.488, "loss": "4.444", "nll_loss": "2.739", "ppl": "6.67", "wps": "3390.8", "ups": "5.06", "wpb": "670.7", "bsz": "32", "num_updates": "7600", "lr": "4.96697e-05", "gnorm": "3.752", "train_wall": "20", "wall": "1874"}
2022-08-18 20:20:09 | INFO | train_inner | {"epoch": 4, "update": 3.534, "loss": "4.403", "nll_loss": "2.688", "ppl": "6.44", "wps": "3361.2", "ups": "4.96", "wpb": "677.3", "bsz": "32", "num_updates": "7700", "lr": "4.96647e-05", "gnorm": "3.863", "train_wall": "20", "wall": "1895"}
2022-08-18 20:20:28 | INFO | train_inner | {"epoch": 4, "update": 3.58, "loss": "4.461", "nll_loss": "2.763", "ppl": "6.79", "wps": "3515.7", "ups": "5.39", "wpb": "651.9", "bsz": "32", "num_updates": "7800", "lr": "4.96597e-05", "gnorm": "3.856", "train_wall": "18", "wall": "1913"}
2022-08-18 20:20:49 | INFO | train_inner | {"epoch": 4, "update": 3.626, "loss": "4.55", "nll_loss": "2.864", "ppl": "7.28", "wps": "3246.9", "ups": "4.89", "wpb": "664.2", "bsz": "32", "num_updates": "7900", "lr": "4.96547e-05", "gnorm": "3.851", "train_wall": "20", "wall": "1934"}
2022-08-18 20:21:07 | INFO | train_inner | {"epoch": 4, "update": 3.671, "loss": "4.43", "nll_loss": "2.726", "ppl": "6.62", "wps": "3266.1", "ups": "5.43", "wpb": "602", "bsz": "32", "num_updates": "8000", "lr": "4.96496e-05", "gnorm": "3.926", "train_wall": "18", "wall": "1952"}
2022-08-18 20:21:31 | INFO | train_inner | {"epoch": 4, "update": 3.717, "loss": "4.597", "nll_loss": "2.916", "ppl": "7.55", "wps": "3036.4", "ups": "4.21", "wpb": "721.9", "bsz": "31.8", "num_updates": "8100", "lr": "4.96446e-05", "gnorm": "3.765", "train_wall": "24", "wall": "1976"}
2022-08-18 20:21:49 | INFO | train_inner | {"epoch": 4, "update": 3.763, "loss": "4.392", "nll_loss": "2.684", "ppl": "6.42", "wps": "3497.5", "ups": "5.44", "wpb": "643.4", "bsz": "32", "num_updates": "8200", "lr": "4.96396e-05", "gnorm": "3.806", "train_wall": "18", "wall": "1994"}
2022-08-18 20:22:09 | INFO | train_inner | {"epoch": 4, "update": 3.809, "loss": "4.581", "nll_loss": "2.9", "ppl": "7.46", "wps": "3439.3", "ups": "5.15", "wpb": "668.4", "bsz": "32", "num_updates": "8300", "lr": "4.96346e-05", "gnorm": "3.897", "train_wall": "19", "wall": "2014"}
2022-08-18 20:22:27 | INFO | train_inner | {"epoch": 4, "update": 3.855, "loss": "4.527", "nll_loss": "2.842", "ppl": "7.17", "wps": "3429.8", "ups": "5.47", "wpb": "626.6", "bsz": "32", "num_updates": "8400", "lr": "4.96296e-05", "gnorm": "3.882", "train_wall": "18", "wall": "2032"}
2022-08-18 20:22:48 | INFO | train_inner | {"epoch": 4, "update": 3.901, "loss": "4.412", "nll_loss": "2.708", "ppl": "6.53", "wps": "3008.8", "ups": "4.66", "wpb": "646.3", "bsz": "32", "num_updates": "8500", "lr": "4.96246e-05", "gnorm": "3.876", "train_wall": "21", "wall": "2053"}
2022-08-18 20:23:06 | INFO | train_inner | {"epoch": 4, "update": 3.947, "loss": "4.471", "nll_loss": "2.781", "ppl": "6.87", "wps": "3642.5", "ups": "5.8", "wpb": "628", "bsz": "32", "num_updates": "8600", "lr": "4.96196e-05", "gnorm": "3.843", "train_wall": "17", "wall": "2071"}
2022-08-18 20:23:26 | INFO | train_inner | {"epoch": 4, "update": 3.993, "loss": "4.67", "nll_loss": "3.006", "ppl": "8.03", "wps": "3370.8", "ups": "4.85", "wpb": "695.6", "bsz": "32", "num_updates": "8700", "lr": "4.96146e-05", "gnorm": "3.821", "train_wall": "20", "wall": "2091"}
2022-08-18 20:23:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 20:25:38 | INFO | valid | {"epoch": 4, "valid_loss": "4.962", "valid_nll_loss": "3.252", "valid_ppl": "9.52", "valid_bleu": "15.69", "valid_wps": "1380.4", "valid_wpb": "652.8", "valid_bsz": "31.9", "valid_num_updates": "8716", "valid_best_bleu": "15.69"}
2022-08-18 20:25:38 | INFO | fairseq_cli.train | begin save checkpoint
2022-08-18 20:25:45 | INFO | fairseq.checkpoint_utils | saved checkpoint /home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_java_en_XX/checkpoint_best.pt (epoch 4 @ 8716 updates, score 15.69) (writing took 6.739846557378769 seconds)
2022-08-18 20:25:45 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-08-18 20:25:45 | INFO | train | {"epoch": 4, "train_loss": "4.469", "train_nll_loss": "2.768", "train_ppl": "6.81", "train_wps": "2526.9", "train_ups": "3.87", "train_wpb": "653", "train_bsz": "32", "train_num_updates": "8716", "train_lr": "4.96138e-05", "train_gnorm": "3.793", "train_train_wall": "422", "train_wall": "2230"}
2022-08-18 20:25:45 | INFO | fairseq.trainer | begin training epoch 5
2022-08-18 20:26:01 | INFO | train_inner | {"epoch": 5, "update": 4.039, "loss": "4.022", "nll_loss": "2.254", "ppl": "4.77", "wps": "410.6", "ups": "0.65", "wpb": "634.4", "bsz": "32", "num_updates": "8800", "lr": "4.96096e-05", "gnorm": "3.665", "train_wall": "17", "wall": "2246"}
2022-08-18 20:26:21 | INFO | train_inner | {"epoch": 5, "update": 4.084, "loss": "4.217", "nll_loss": "2.475", "ppl": "5.56", "wps": "3292.9", "ups": "4.81", "wpb": "684.8", "bsz": "32", "num_updates": "8900", "lr": "4.96046e-05", "gnorm": "3.702", "train_wall": "21", "wall": "2267"}
2022-08-18 20:26:40 | INFO | train_inner | {"epoch": 5, "update": 4.13, "loss": "4.167", "nll_loss": "2.418", "ppl": "5.34", "wps": "3540.5", "ups": "5.26", "wpb": "673.2", "bsz": "32", "num_updates": "9000", "lr": "4.95996e-05", "gnorm": "3.813", "train_wall": "19", "wall": "2286"}
2022-08-18 20:27:00 | INFO | train_inner | {"epoch": 5, "update": 4.176, "loss": "4.163", "nll_loss": "2.413", "ppl": "5.33", "wps": "3414.6", "ups": "5.06", "wpb": "675", "bsz": "32", "num_updates": "9100", "lr": "4.95946e-05", "gnorm": "3.791", "train_wall": "20", "wall": "2305"}
2022-08-18 20:27:21 | INFO | train_inner | {"epoch": 5, "update": 4.222, "loss": "4.283", "nll_loss": "2.553", "ppl": "5.87", "wps": "3574.2", "ups": "4.86", "wpb": "735.8", "bsz": "32", "num_updates": "9200", "lr": "4.95896e-05", "gnorm": "3.697", "train_wall": "20", "wall": "2326"}
2022-08-18 20:27:41 | INFO | train_inner | {"epoch": 5, "update": 4.268, "loss": "4.127", "nll_loss": "2.377", "ppl": "5.19", "wps": "3219.3", "ups": "4.92", "wpb": "653.7", "bsz": "32", "num_updates": "9300", "lr": "4.95846e-05", "gnorm": "3.839", "train_wall": "20", "wall": "2346"}
2022-08-18 20:28:02 | INFO | train_inner | {"epoch": 5, "update": 4.314, "loss": "4.233", "nll_loss": "2.497", "ppl": "5.65", "wps": "3186.3", "ups": "4.8", "wpb": "663.7", "bsz": "32", "num_updates": "9400", "lr": "4.95796e-05", "gnorm": "3.875", "train_wall": "21", "wall": "2367"}
2022-08-18 20:28:19 | INFO | train_inner | {"epoch": 5, "update": 4.36, "loss": "4.002", "nll_loss": "2.236", "ppl": "4.71", "wps": "3428.8", "ups": "5.72", "wpb": "599.1", "bsz": "32", "num_updates": "9500", "lr": "4.95746e-05", "gnorm": "3.958", "train_wall": "17", "wall": "2384"}
2022-08-18 20:28:40 | INFO | train_inner | {"epoch": 5, "update": 4.406, "loss": "4.201", "nll_loss": "2.463", "ppl": "5.51", "wps": "3255.7", "ups": "4.97", "wpb": "654.8", "bsz": "32", "num_updates": "9600", "lr": "4.95696e-05", "gnorm": "3.877", "train_wall": "20", "wall": "2405"}
2022-08-18 20:28:59 | INFO | train_inner | {"epoch": 5, "update": 4.452, "loss": "3.99", "nll_loss": "2.223", "ppl": "4.67", "wps": "3046.4", "ups": "5.28", "wpb": "577.4", "bsz": "32", "num_updates": "9700", "lr": "4.95646e-05", "gnorm": "3.982", "train_wall": "19", "wall": "2424"}
2022-08-18 20:29:19 | INFO | train_inner | {"epoch": 5, "update": 4.497, "loss": "4.331", "nll_loss": "2.614", "ppl": "6.12", "wps": "3436.9", "ups": "4.94", "wpb": "695.8", "bsz": "32", "num_updates": "9800", "lr": "4.95596e-05", "gnorm": "3.899", "train_wall": "20", "wall": "2444"}
2022-08-18 20:29:38 | INFO | train_inner | {"epoch": 5, "update": 4.543, "loss": "4.103", "nll_loss": "2.353", "ppl": "5.11", "wps": "3345.6", "ups": "5.33", "wpb": "627.9", "bsz": "32", "num_updates": "9900", "lr": "4.95546e-05", "gnorm": "4.043", "train_wall": "19", "wall": "2463"}
2022-08-18 20:29:57 | INFO | train_inner | {"epoch": 5, "update": 4.589, "loss": "4.26", "nll_loss": "2.531", "ppl": "5.78", "wps": "3361.6", "ups": "5.08", "wpb": "661.7", "bsz": "32", "num_updates": "10000", "lr": "4.95495e-05", "gnorm": "4.11", "train_wall": "19", "wall": "2482"}
2022-08-18 20:30:17 | INFO | train_inner | {"epoch": 5, "update": 4.635, "loss": "4.19", "nll_loss": "2.456", "ppl": "5.49", "wps": "3116.7", "ups": "5.13", "wpb": "607.4", "bsz": "32", "num_updates": "10100", "lr": "4.95445e-05", "gnorm": "4.046", "train_wall": "19", "wall": "2502"}
2022-08-18 20:30:33 | INFO | train_inner | {"epoch": 5, "update": 4.681, "loss": "4.23", "nll_loss": "2.501", "ppl": "5.66", "wps": "3829.7", "ups": "6.13", "wpb": "624.9", "bsz": "32", "num_updates": "10200", "lr": "4.95395e-05", "gnorm": "4.018", "train_wall": "16", "wall": "2518"}
2022-08-18 20:30:52 | INFO | train_inner | {"epoch": 5, "update": 4.727, "loss": "4.072", "nll_loss": "2.325", "ppl": "5.01", "wps": "3159.1", "ups": "5.26", "wpb": "600.2", "bsz": "32", "num_updates": "10300", "lr": "4.95345e-05", "gnorm": "4.032", "train_wall": "19", "wall": "2537"}
2022-08-18 20:31:13 | INFO | train_inner | {"epoch": 5, "update": 4.773, "loss": "4.235", "nll_loss": "2.503", "ppl": "5.67", "wps": "3213.2", "ups": "4.74", "wpb": "678.6", "bsz": "32", "num_updates": "10400", "lr": "4.95295e-05", "gnorm": "4.009", "train_wall": "21", "wall": "2558"}
2022-08-18 20:31:33 | INFO | train_inner | {"epoch": 5, "update": 4.819, "loss": "4.188", "nll_loss": "2.455", "ppl": "5.48", "wps": "3188.4", "ups": "4.95", "wpb": "644.5", "bsz": "32", "num_updates": "10500", "lr": "4.95245e-05", "gnorm": "4.011", "train_wall": "20", "wall": "2578"}
2022-08-18 20:31:52 | INFO | train_inner | {"epoch": 5, "update": 4.865, "loss": "4.222", "nll_loss": "2.495", "ppl": "5.64", "wps": "3578.4", "ups": "5.3", "wpb": "675.5", "bsz": "31.8", "num_updates": "10600", "lr": "4.95195e-05", "gnorm": "3.942", "train_wall": "19", "wall": "2597"}
2022-08-18 20:32:11 | INFO | train_inner | {"epoch": 5, "update": 4.911, "loss": "4.18", "nll_loss": "2.446", "ppl": "5.45", "wps": "3499.2", "ups": "5.37", "wpb": "651.8", "bsz": "32", "num_updates": "10700", "lr": "4.95145e-05", "gnorm": "4.058", "train_wall": "18", "wall": "2616"}
2022-08-18 20:32:33 | INFO | train_inner | {"epoch": 5, "update": 4.956, "loss": "4.287", "nll_loss": "2.568", "ppl": "5.93", "wps": "3063.8", "ups": "4.6", "wpb": "665.9", "bsz": "32", "num_updates": "10800", "lr": "4.95095e-05", "gnorm": "4.065", "train_wall": "22", "wall": "2638"}
2022-08-18 20:32:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 20:35:13 | INFO | valid | {"epoch": 5, "valid_loss": "4.938", "valid_nll_loss": "3.224", "valid_ppl": "9.34", "valid_bleu": "20.48", "valid_wps": "1264.9", "valid_wpb": "652.8", "valid_bsz": "31.9", "valid_num_updates": "10895", "valid_best_bleu": "20.48"}
2022-08-18 20:35:13 | INFO | fairseq_cli.train | begin save checkpoint
2022-08-18 20:35:20 | INFO | fairseq.checkpoint_utils | saved checkpoint /home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_java_en_XX/checkpoint_best.pt (epoch 5 @ 10895 updates, score 20.48) (writing took 6.917357794940472 seconds)
2022-08-18 20:35:20 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-08-18 20:35:20 | INFO | train | {"epoch": 5, "train_loss": "4.18", "train_nll_loss": "2.44", "train_ppl": "5.43", "train_wps": "2473.1", "train_ups": "3.79", "train_wpb": "653", "train_bsz": "32", "train_num_updates": "10895", "train_lr": "4.95048e-05", "train_gnorm": "3.93", "train_train_wall": "422", "train_wall": "2805"}
2022-08-18 20:35:20 | INFO | fairseq_cli.train | done training in 2798.2 seconds
@ Completed
@ Stage 3
Create Datastore
2022-08-18 20:35:22 | INFO | fairseq_cli.validate | loading model(s) from /home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_java_en_XX/checkpoint_best.pt
2022-08-18 20:35:24 | INFO | fairseq.tasks.translation | [java] dictionary: 50001 types
2022-08-18 20:35:24 | INFO | fairseq.tasks.translation | [en_XX] dictionary: 50001 types
2022-08-18 20:35:32 | INFO | fairseq_cli.validate | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='mbart_base', attention_dropout=0.0, batch_size=32, batch_size_valid=32, best_checkpoint_metric='bleu', bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin', data_buffer_size=10, dataset_impl='mmap', ddp_backend='no_c10d', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, dstore_filename=None, dstore_fp16=True, dstore_mmap='/home/cluster/jgu/scratch/ssr/cli/out/mix/java/8th_datastore', dstore_size=1283405, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eval_bleu=True, eval_bleu_args='{"beam": 6}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, k=5, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, knn_lambda=0.5, knn_sim_metric=None, knn_temperature=10, label_smoothing=0.1, langs='java,python,en_XX', layernorm_embedding=True, left_pad_source=False, left_pad_target=False, load_alignments=False, localsgd_frequency=3, log_format='json', log_interval=100, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, move_dstore_to_mem=False, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, only_train_final_output=False, optimizer='adam', optimizer_overrides='{}', partially_finetune=False, path='/home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_java_en_XX/checkpoint_best.pt', patience=5, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, prepend_bos=False, probe=32, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='/home/cluster/jgu/scratch/ssr/cli/model/plbart_base.pt', save_dir='/home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_java_en_XX', save_interval=1, save_interval_updates=0, scoring='bleu', seed=42, sentence_avg=False, sentencepiece_model='/home/cluster/jgu/scratch/ssr/cli/sentencepiece/sentencepiece.bpe.model', shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='java', stop_time_hours=0, target_lang='en_XX', task='translation_from_pretrained_bart', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_gpu_to_search=False, use_knn_datastore=False, use_old_adam=False, user_dir='/home/cluster/jgu/scratch/ssr/cli', valid_subset='train', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=1000, weight_decay=0.0, zero_sharding='none')
Saving fp16
2022-08-18 20:35:32 | INFO | fairseq.data.data_utils | loaded 69708 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin/train.java-en_XX.java
2022-08-18 20:35:32 | INFO | fairseq.data.data_utils | loaded 69708 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin/train.java-en_XX.en_XX
2022-08-18 20:35:32 | INFO | fairseq.tasks.translation | /home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin train java-en_XX 69708 examples
/home/cluster/jgu/scratch/ssr/cli/ds_build.py:69: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dstore_vals = np.memmap(args.dstore_mmap + '/vals.npy', dtype=np.int, mode='w+',
/home/cluster/jgu/scratch/ssr/cli/ds_build.py:157: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dstore_vals[dstore_idx:reduce_size + dstore_idx] = target.unsqueeze(-1).cpu().numpy().astype(np.int)
992
1577
2545
3134
3784
5053
5675
7164
7768
9661
10373
12500
13643
16302
17786
18573
21222
23339
24067
26192
28608
30060
31928
34793
37128
38171
40039
42624
45695
47585
49075
50757
53051
55662
57777
58615
60244
62387
65186
67962
70121
71176
72660
74530
77173
79814
81561
82074
83576
85504
87983
90900
93334
94771
95975
97535
99519
102154
105058
107128
107795
108964
110553
112677
115470
117830
118948
120243
121958
124183
126952
129471
129826
131033
132670
134852
137860
140209
141985
143101
144571
146436
149128
151642
152469
153639
155184
157190
159943
162703
165168
166352
167470
169008
170969
173788
176253
177069
178135
179488
181218
183635
186293
188098
189077
190369
192071
194620
196613
197918
198995
200427
202326
205191
207366
208383
209308
210471
211865
213737
216434
218304
219133
220088
221340
222950
225147
227565
228806
229703
230777
232119
234005
236396
237736
238583
239703
241076
242937
245489
247781
248248
249174
250278
251615
253365
256127
258158
258617
259458
260562
262035
263950
266536
268253
269045
270093
271436
273232
275828
277244
278028
279083
280432
282269
284601
285747
286460
287418
288641
290256
292514
294355
295257
296028
296986
298188
299641
301505
303399
304952
305704
306696
307901
309452
311326
313916
315248
315969
316909
318096
319664
322192
324462
324972
325656
326536
327616
328972
330736
332948
334699
335346
336168
337195
338477
340121
342711
344875
345498
346302
347268
348414
349857
351905
354044
354665
355401
356267
357330
358659
360221
362667
364803
365611
366332
367232
368323
369781
371923
373950
374886
375478
376225
377116
378230
379772
382113
384121
384695
385445
386344
387503
389046
391369
392642
393229
393939
394792
395836
397288
399399
401504
402092
402824
403729
404832
406287
408174
410023
411423
412051
412836
413812
415064
416650
418812
421091
421572
422217
423015
423892
424962
426335
428262
430408
431248
431820
432521
433372
434413
435869
438057
439541
440019
440636
441355
442172
443173
444397
446116
448138
448508
449073
449753
450550
451505
452653
454192
456266
456726
457209
457814
458577
459468
460621
462197
464134
465007
465566
466234
467037
468000
469191
470886
473410
474359
474943
475666
476494
477517
478956
480863
481729
482339
483085
483990
485222
487197
489552
489950
490450
491059
491780
492629
493655
494932
496818
498990
499431
500000
500678
501534
502629
504102
506460
509092
511153
511571
512119
512786
513592
514586
515963
518326
519302
519779
520356
521052
521891
522902
524391
526367
527514
528016
528637
529397
530306
531467
533185
534470
534960
535544
536191
536953
537882
539148
541183
542361
542872
543479
544185
545008
546028
547148
548268
549388
550522
552281
554029
554504
555095
555807
556615
557556
558762
560895
562503
563044
563731
564539
565522
566909
569277
570668
571066
571536
572090
572725
573480
574371
575491
577293
578563
578962
579434
580005
580680
581545
582678
584720
585104
585612
586231
586993
588036
589606
590786
591197
591685
592246
592900
593711
594799
596487
597897
598285
598772
599341
599997
600787
601690
602925
604889
605977
606416
606941
607554
608303
609231
610380
612165
612898
613346
613891
614544
615334
616324
617727
619673
620057
620558
621183
621918
622825
624037
626162
626990
627441
627982
628611
629380
630365
631674
633790
634395
634827
635337
635891
636551
637345
638360
639677
641642
642007
642470
643018
643659
644429
645363
646612
648614
649853
650286
650810
651470
652273
653328
654954
656061
656419
656871
657390
657971
658669
659572
660704
662579
664109
664473
664920
665464
666113
666920
667966
670103
670522
670947
671445
672028
672733
673542
674584
676307
678985
679299
679682
680118
680638
681289
682101
683174
685313
685698
686067
686515
687035
687605
688274
689027
689975
691414
693013
693371
693771
694171
694600
695106
695705
696439
697351
698479
699821
701804
702125
702523
703007
703592
704342
705300
706653
707836
708225
708696
709250
709937
710767
711822
713449
714668
715022
715453
715975
716616
717402
718415
719926
720971
721346
721792
722336
723006
723895
725220
727111
727476
727914
728472
729164
729999
731008
732650
733580
733956
734418
735010
735760
736767
738372
740261
740608
741034
741552
742168
742885
743747
744897
746891
747206
747607
748092
748665
749380
750316
751835
753271
753651
754121
754726
755499
756582
758682
759098
759540
760059
760711
761622
763010
764297
764673
765151
765726
766398
767266
768494
770414
770649
770937
771275
771651
772097
772577
773085
773675
774431
775523
776981
777269
777610
778009
778489
779091
779905
781132
782071
782358
782703
783087
783471
783868
784335
784894
785541
786396
787889
788469
788783
789157
789581
790128
790854
791829
793372
794005
794337
794711
795161
795692
796326
797193
798586
799189
799481
799827
800251
800787
801480
802322
803491
804443
804752
805132
805590
806159
806942
808036
809532
809797
810117
810507
810967
811515
812261
813312
814783
815062
815421
815856
816391
817087
817871
818951
820688
820949
821285
821700
822208
822838
823622
824708
826471
826728
827067
827451
827835
828253
828740
829434
830456
831916
832196
832548
832972
833458
834049
834802
835963
836959
837231
837571
837984
838496
839170
840110
841313
842848
843138
843519
843994
844662
845593
846924
847165
847486
847857
848318
848918
849782
851193
851971
852306
852707
853210
853875
854852
856560
856804
857136
857539
858014
858594
859339
860492
861608
861900
862245
862664
863145
863731
864562
866343
867558
867858
868229
868678
869223
869935
870982
872355
872630
872964
873393
873959
874749
876027
876878
877134
877441
877817
878251
878812
879535
880527
882293
882565
882900
883312
883850
884586
886085
887039
887345
887749
888272
888971
889959
891517
892735
893029
893403
893919
894577
895644
896465
896752
897123
897599
898227
899055
900383
901308
901628
902023
902503
903085
903823
904711
905911
906467
906674
906928
907240
907602
908055
908599
909271
910395
911328
911569
911861
912219
912657
913234
913951
915109
915704
915921
916185
916501
916899
917418
918145
919316
920302
920499
920746
921033
921360
921754
922235
922790
923508
924537
925561
925775
926045
926353
926710
927134
927634
928270
929100
929763
929965
930199
930474
930825
931247
931792
932487
933450
934501
934701
934941
935248
935608
936058
936617
937345
938466
938662
938940
939279
939700
940237
941097
941896
942135
942435
942772
943242
943847
944650
946014
946217
946488
946814
947200
947680
948302
949186
950516
950707
950965
951288
951672
952155
952798
953779
954375
954613
954923
955328
955851
956632
958223
958411
958670
958985
959350
959807
960401
961291
962262
962474
962763
963103
963539
964118
965187
965766
966006
966325
966758
967343
968259
969387
970675
971246
971467
971741
972070
972459
972929
973571
974590
975139
975355
975599
975904
976253
976713
977270
977878
978890
979237
979486
979786
980164
980646
981239
982119
983326
983548
983820
984180
984642
985288
986227
986434
986693
987009
987439
988008
988751
989701
989927
990208
990560
991062
991804
992954
993172
993491
993986
994739
996256
996473
996731
997079
997520
997992
998639
999533
1000618
1000842
1001126
1001482
1001974
1002789
1003716
1004740
1004957
1005228
1005574
1006009
1006581
1007473
1008351
1008569
1008849
1009201
1009641
1010243
1011098
1012070
1012301
1012607
1013056
1013714
1014617
1014879
1015213
1015627
1016146
1017252
1017566
1017858
1018237
1018735
1019487
1020556
1020750
1020981
1021287
1021661
1022232
1023184
1023823
1024113
1024592
1025375
1026652
1026871
1027166
1027550
1028030
1028768
1029704
1029952
1030290
1030733
1031355
1032498
1033038
1033312
1033676
1034242
1035345
1035918
1036192
1036559
1037119
1037953
1038765
1039011
1039364
1039817
1040487
1041740
1042010
1042368
1042880
1043788
1044330
1044589
1044942
1045482
1046459
1046889
1047167
1047534
1047983
1048609
1049602
1049835
1050155
1050539
1050984
1051598
1052726
1052947
1053249
1053655
1054257
1055193
1055417
1055729
1056198
1056921
1057731
1058283
1058422
1058597
1058803
1059041
1059339
1059702
1060234
1061353
1061853
1062023
1062230
1062487
1062803
1063223
1063825
1064465
1064587
1064742
1064930
1065164
1065483
1065953
1066971
1067560
1067712
1067896
1068135
1068441
1068830
1069338
1070123
1070247
1070423
1070653
1070948
1071315
1071831
1072960
1073186
1073374
1073599
1073871
1074276
1074912
1075467
1075619
1075796
1076034
1076337
1076802
1077652
1078260
1078414
1078601
1078827
1079101
1079515
1080263
1080546
1080728
1080957
1081265
1081678
1082305
1083022
1083163
1083337
1083562
1083839
1084224
1084896
1085471
1085631
1085834
1086073
1086389
1086866
1087445
1087588
1087770
1087971
1088233
1088609
1089207
1089742
1089887
1090080
1090318
1090609
1090996
1091463
1092229
1092490
1092653
1092847
1093088
1093405
1093834
1094420
1095256
1095391
1095565
1095782
1096060
1096403
1096852
1097497
1098225
1098375
1098569
1098795
1099087
1099498
1100107
1100870
1101012
1101186
1101391
1101615
1101848
1102167
1102610
1103307
1104185
1104335
1104533
1104808
1105214
1106064
1106287
1106477
1106699
1107004
1107499
1108165
1108334
1108548
1108843
1109361
1110342
1110659
1110832
1111052
1111345
1111772
1112484
1113026
1113205
1113455
1113786
1114314
1114995
1115143
1115338
1115634
1116024
1116579
1117406
1117545
1117732
1117987
1118364
1118885
1119440
1119610
1119848
1120184
1120753
1121419
1121551
1121732
1121966
1122308
1122917
1123691
1123846
1124085
1124471
1125099
1125978
1126203
1126402
1126717
1127167
1127905
1128044
1128191
1128363
1128587
1128907
1129390
1130149
1131144
1131291
1131475
1131717
1132072
1132490
1133248
1133393
1133587
1133884
1134355
1134881
1135056
1135317
1135717
1136275
1136758
1136928
1137169
1137510
1138100
1138792
1138964
1139201
1139547
1140082
1140584
1140755
1140947
1141177
1141480
1141866
1142537
1143106
1143258
1143467
1143750
1144122
1144657
1144988
1145163
1145375
1145656
1146057
1146759
1147179
1147356
1147598
1147981
1148607
1148887
1149108
1149406
1149764
1150258
1151133
1152010
1152192
1152464
1152850
1153506
1153861
1154040
1154276
1154585
1155146
1155844
1156012
1156221
1156506
1156955
1157671
1157811
1157997
1158228
1158573
1159115
1159758
1160347
1160542
1160890
1161590
1162054
1162245
1162480
1162897
1163675
1163815
1164017
1164330
1164906
1165052
1165240
1165511
1165903
1166653
1167277
1167443
1167664
1167992
1168647
1169224
1169394
1169623
1170035
1170759
1170898
1171110
1171445
1172024
1172457
1172804
1173511
1173648
1173860
1174149
1174784
1175118
1175337
1175645
1176148
1176735
1176940
1177249
1177727
1178776
1178932
1179139
1179441
1179983
1180407
1180567
1180795
1181109
1181505
1182193
1182339
1182552
1182897
1183515
1183664
1183862
1184129
1184626
1185109
1185252
1185442
1185708
1186320
1186455
1186635
1186892
1187294
1188031
1188171
1188348
1188637
1189112
1189991
1190122
1190276
1190466
1190707
1191051
1191569
1192299
1192449
1192671
1193075
1193890
1194035
1194240
1194555
1195107
1195273
1195525
1195913
1196693
1196928
1197143
1197452
1198169
1198327
1198568
1199030
1200109
1200234
1200394
1200616
1200963
1201495
1201675
1201985
1202419
1202974
1203149
1203460
1204265
1204391
1204605
1204996
1205705
1205881
1206248
1206629
1206819
1207073
1207665
1208061
1208258
1208599
1209144
1209704
1210503
1210658
1210879
1211271
1211848
1212020
1212280
1212748
1213494
1213680
1213969
1214344
1215310
1215479
1215736
1216200
1216718
1216914
1217391
1218162
1218352
1218629
1219349
1219691
1219897
1220349
1220802
1220983
1221253
1221784
1222119
1222353
1222757
1223170
1223380
1223761
1224731
1224870
1225066
1225543
1225906
1226093
1226409
1226673
1227035
1227108
1227218
1227376
1227572
1227893
1228235
1228304
1228390
1228500
1228618
1228754
1228890
1229026
1229191
1229512
1229944
1230014
1230101
1230220
1230410
1230722
1231320
1231380
1231460
1231565
1231687
1231868
1232115
1232413
1232844
1233079
1233157
1233249
1233381
1233553
1233776
1234032
1234227
1234317
1234427
1234550
1234691
1234846
1235034
1235315
1235672
1235752
1235869
1236036
1236309
1236692
1236785
1236911
1237065
1237311
1237777
1237851
1237954
1238108
1238452
1238527
1238627
1238780
1239032
1239261
1239336
1239437
1239569
1239740
1239977
1240394
1240465
1240561
1240707
1240943
1241468
1241712
1241805
1241921
1242054
1242235
1242533
1243089
1243167
1243274
1243414
1243594
1243857
1244414
1244658
1244745
1244879
1245097
1245631
1245699
1245785
1245921
1246131
1246426
1246953
1247045
1247278
1247594
1247708
1247863
1248070
1248376
1248603
1248674
1248776
1248902
1249075
1249318
1249748
1250430
1250507
1250611
1250741
1250912
1251184
1251335
1251417
1251520
1251668
1251913
1252456
1252516
1252605
1252734
1252920
1253348
1253696
1253786
1253921
1254121
1254419
1254614
1254702
1254837
1255026
1255422
1255487
1255613
1255777
1256225
1256288
1256393
1256533
1256813
1257215
1257679
1257960
1258037
1258143
1258314
1258638
1258747
1258833
1258962
1259179
1259550
1259629
1259741
1259871
1260031
1260356
1260533
1260628
1260753
1260989
1261457
1261605
1261727
1261935
1262404
1262467
1262560
1262702
1262895
1263386
1263464
1263561
1263678
1263838
1264093
1264486
1264697
1264785
1264873
1264983
1265102
1265301
1265638
1266229
1266294
1266378
1266491
1266660
1266863
1267303
1267368
1267463
1267574
1267755
1268057
1268132
1268243
1268405
1268945
1269198
1269293
1269400
1269524
1269759
1269836
1269939
1270077
1270245
1270413
1270648
1270888
1270950
1271046
1271176
1271358
1271635
1271834
1271904
1272008
1272139
1272329
1272727
1272882
1272973
1273117
1273300
1273534
1273975
1274506
1274579
1274680
1274867
1275048
1275124
1275219
1275334
1275523
1275891
1276170
1276274
1276472
1276828
1277148
1277237
1277364
1277579
1277666
1277815
1278100
1278464
1278553
1278702
1278945
1279404
1279569
1279669
1279809
1280203
1280287
1280391
1280549
1280922
1280989
1281092
1281252
1281602
1282022
1282123
1282269
1282519
1282919
1283164
1283306
1283405
much more than dstore size break
Build Faiss Index
/home/cluster/jgu/scratch/ssr/cli/ds_train.py:28: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  vals = np.memmap(args.dstore_mmap + '/vals.npy', dtype=np.int, mode='r', shape=(args.dstore_size, 1))
Namespace(code_size=64, dimension=768, dstore_fp16=True, dstore_mmap='/home/cluster/jgu/scratch/ssr/cli/out/mix/java/8th_datastore', dstore_size=1283405, faiss_index='/home/cluster/jgu/scratch/ssr/cli/out/mix/java/8th_datastore/knn_index', ncentroids=4096, probe=32, seed=1, starting_point=0)
done.
Start put index to gpu
Training Index
[1254121 1090986  296881  979708 1012452 1221391  554957  845181 1000300
   26129]
[[-0.9375  -0.7466  -1.202   ...  3.889    0.791    1.012  ]
 [ 1.626   -1.023   -3.207   ... -2.277    3.982   -1.316  ]
 [-0.6343   2.312   -0.807   ... -1.626    0.4592  -3.336  ]
 ...
 [-5.867    0.3284  -0.2485  ... -0.222    1.496    0.6875 ]
 [-4.15     1.647   -1.836   ... -2.363    1.588   -0.2491 ]
 [ 0.4941  -1.303   -1.171   ...  0.1284  -0.10254 -1.283  ]]
Training took 28.636061668395996 s
Writing index after training
Writing index took 0.0210878849029541 s
Adding Keys
Added 1000000 tokens so far
Writing Index 1000000
Adding total 1283405 keys
Adding took 8.683434009552002 s
Writing Index
Writing index took 0.261319637298584 s
@ Completed
@ Stage 4
/net/cephfs/scratch/jgu/ssr/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beams_buf = indices_buf // vocab_size
/net/cephfs/scratch/jgu/ssr/fairseq/sequence_generator.py:651: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  unfin_idx = idx // beam_size
WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.
c_bleu = 20.07 | s_bleu = 34.51 | meteor = 19.56 | rouge = 47.98
@ Completed

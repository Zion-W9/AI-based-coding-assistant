@ Stage 2
2022-08-18 19:49:12 | INFO | fairseq_cli.train | Namespace(activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='mbart_base', attention_dropout=0.0, batch_size=32, batch_size_valid=32, best_checkpoint_metric='bleu', bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/home/cluster/jgu/scratch/ssr/cli/out/mix/python/data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, dstore_filename=None, dstore_size=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eval_bleu=True, eval_bleu_args='{"beam": 6}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, k=5, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, knn_lambda=0.5, knn_sim_metric=None, knn_temperature=10, label_smoothing=0.1, langs='java,python,en_XX', layernorm_embedding=True, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format='json', log_interval=100, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=4, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, move_dstore_to_mem=False, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, only_train_final_output=False, optimizer='adam', optimizer_overrides='{}', partially_finetune=False, patience=5, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, prepend_bos=False, probe=32, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='/home/cluster/jgu/scratch/ssr/cli/model/plbart_base.pt', save_dir='/home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_python_en_XX', save_interval=1, save_interval_updates=0, scoring='bleu', seed=42, sentence_avg=False, sentencepiece_model='/home/cluster/jgu/scratch/ssr/cli/sentencepiece/sentencepiece.bpe.model', shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='python', stop_time_hours=0, target_lang='en_XX', task='translation_from_pretrained_bart', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_gpu_to_search=False, use_knn_datastore=False, use_old_adam=False, user_dir='/home/cluster/jgu/scratch/ssr/cli', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=1000, weight_decay=0.0, zero_sharding='none')
2022-08-18 19:49:12 | INFO | fairseq.tasks.translation | [python] dictionary: 50001 types
2022-08-18 19:49:12 | INFO | fairseq.tasks.translation | [en_XX] dictionary: 50001 types
2022-08-18 19:49:12 | INFO | fairseq.data.data_utils | loaded 18505 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/python/data-bin/valid.python-en_XX.python
2022-08-18 19:49:13 | INFO | fairseq.data.data_utils | loaded 18505 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/python/data-bin/valid.python-en_XX.en_XX
2022-08-18 19:49:13 | INFO | fairseq.tasks.translation | /home/cluster/jgu/scratch/ssr/cli/out/mix/python/data-bin valid python-en_XX 18505 examples
2022-08-18 19:49:16 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=50005, bias=False)
  )
  (classification_heads): ModuleDict()
)
2022-08-18 19:49:16 | INFO | fairseq_cli.train | task: translation_from_pretrained_bart (TranslationFromPretrainedBARTTask)
2022-08-18 19:49:16 | INFO | fairseq_cli.train | model: mbart_base (BARTModel)
2022-08-18 19:49:16 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2022-08-18 19:49:16 | INFO | fairseq_cli.train | num. model params: 139220736 (num. trained: 139220736)
2022-08-18 19:49:20 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-08-18 19:49:20 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-08-18 19:49:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-08-18 19:49:20 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM2-32GB                    
2022-08-18 19:49:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-08-18 19:49:20 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-08-18 19:49:20 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 32
2022-08-18 19:49:21 | INFO | fairseq.trainer | loaded checkpoint /home/cluster/jgu/scratch/ssr/cli/model/plbart_base.pt (epoch 11 @ 0 updates)
2022-08-18 19:49:21 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2022-08-18 19:49:22 | INFO | fairseq.trainer | loading train data for epoch 1
2022-08-18 19:49:22 | INFO | fairseq.data.data_utils | loaded 55538 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/python/data-bin/train.python-en_XX.python
2022-08-18 19:49:22 | INFO | fairseq.data.data_utils | loaded 55538 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/python/data-bin/train.python-en_XX.en_XX
2022-08-18 19:49:22 | INFO | fairseq.tasks.translation | /home/cluster/jgu/scratch/ssr/cli/out/mix/python/data-bin train python-en_XX 55538 examples
2022-08-18 19:49:22 | INFO | fairseq.trainer | begin training epoch 1
2022-08-18 19:49:35 | INFO | train_inner | {"epoch": 1, "update": 0.058, "loss": "8.876", "nll_loss": "7.16", "ppl": "143.01", "wps": "3280.3", "ups": "7.99", "wpb": "411.6", "bsz": "31.9", "num_updates": "100", "lr": "5e-06", "gnorm": "34.654", "train_wall": "12", "wall": "15"}
2022-08-18 19:49:48 | INFO | train_inner | {"epoch": 1, "update": 0.115, "loss": "6.349", "nll_loss": "4.637", "ppl": "24.88", "wps": "3192", "ups": "7.85", "wpb": "406.5", "bsz": "32", "num_updates": "200", "lr": "1e-05", "gnorm": "6.74", "train_wall": "13", "wall": "28"}
2022-08-18 19:50:00 | INFO | train_inner | {"epoch": 1, "update": 0.173, "loss": "6.009", "nll_loss": "4.355", "ppl": "20.47", "wps": "3402.8", "ups": "8.17", "wpb": "416.3", "bsz": "32", "num_updates": "300", "lr": "1.5e-05", "gnorm": "5.553", "train_wall": "12", "wall": "40"}
2022-08-18 19:50:12 | INFO | train_inner | {"epoch": 1, "update": 0.23, "loss": "5.812", "nll_loss": "4.175", "ppl": "18.07", "wps": "3237.4", "ups": "8.13", "wpb": "398.1", "bsz": "32", "num_updates": "400", "lr": "2e-05", "gnorm": "5.662", "train_wall": "12", "wall": "52"}
2022-08-18 19:50:24 | INFO | train_inner | {"epoch": 1, "update": 0.288, "loss": "5.67", "nll_loss": "4.036", "ppl": "16.4", "wps": "3447.3", "ups": "8.52", "wpb": "404.7", "bsz": "32", "num_updates": "500", "lr": "2.5e-05", "gnorm": "5.09", "train_wall": "12", "wall": "64"}
2022-08-18 19:50:37 | INFO | train_inner | {"epoch": 1, "update": 0.346, "loss": "5.634", "nll_loss": "4.012", "ppl": "16.13", "wps": "3135.1", "ups": "7.79", "wpb": "402.7", "bsz": "32", "num_updates": "600", "lr": "3e-05", "gnorm": "4.907", "train_wall": "13", "wall": "77"}
2022-08-18 19:50:50 | INFO | train_inner | {"epoch": 1, "update": 0.403, "loss": "5.633", "nll_loss": "4.019", "ppl": "16.21", "wps": "3204.6", "ups": "7.59", "wpb": "422.3", "bsz": "32", "num_updates": "700", "lr": "3.5e-05", "gnorm": "4.533", "train_wall": "13", "wall": "90"}
2022-08-18 19:51:03 | INFO | train_inner | {"epoch": 1, "update": 0.461, "loss": "5.555", "nll_loss": "3.939", "ppl": "15.34", "wps": "3190.5", "ups": "7.62", "wpb": "418.6", "bsz": "32", "num_updates": "800", "lr": "4e-05", "gnorm": "4.514", "train_wall": "13", "wall": "103"}
2022-08-18 19:51:16 | INFO | train_inner | {"epoch": 1, "update": 0.518, "loss": "5.47", "nll_loss": "3.853", "ppl": "14.45", "wps": "3066.6", "ups": "7.83", "wpb": "391.5", "bsz": "32", "num_updates": "900", "lr": "4.5e-05", "gnorm": "4.504", "train_wall": "13", "wall": "116"}
2022-08-18 19:51:27 | INFO | train_inner | {"epoch": 1, "update": 0.576, "loss": "5.414", "nll_loss": "3.798", "ppl": "13.9", "wps": "3387.8", "ups": "8.65", "wpb": "391.9", "bsz": "32", "num_updates": "1000", "lr": "5e-05", "gnorm": "4.332", "train_wall": "11", "wall": "128"}
2022-08-18 19:51:40 | INFO | train_inner | {"epoch": 1, "update": 0.634, "loss": "5.443", "nll_loss": "3.831", "ppl": "14.23", "wps": "3194.1", "ups": "7.64", "wpb": "418.2", "bsz": "32", "num_updates": "1100", "lr": "4.9995e-05", "gnorm": "4.264", "train_wall": "13", "wall": "141"}
2022-08-18 19:51:54 | INFO | train_inner | {"epoch": 1, "update": 0.691, "loss": "5.484", "nll_loss": "3.884", "ppl": "14.77", "wps": "3142.1", "ups": "7.41", "wpb": "424.1", "bsz": "32", "num_updates": "1200", "lr": "4.999e-05", "gnorm": "4.022", "train_wall": "13", "wall": "154"}
2022-08-18 19:52:06 | INFO | train_inner | {"epoch": 1, "update": 0.749, "loss": "5.395", "nll_loss": "3.784", "ppl": "13.77", "wps": "3389.1", "ups": "8.17", "wpb": "414.9", "bsz": "32", "num_updates": "1300", "lr": "4.9985e-05", "gnorm": "4.13", "train_wall": "12", "wall": "166"}
2022-08-18 19:52:19 | INFO | train_inner | {"epoch": 1, "update": 0.806, "loss": "5.344", "nll_loss": "3.734", "ppl": "13.31", "wps": "3406.1", "ups": "8.08", "wpb": "421.4", "bsz": "32", "num_updates": "1400", "lr": "4.998e-05", "gnorm": "3.888", "train_wall": "12", "wall": "179"}
2022-08-18 19:52:32 | INFO | train_inner | {"epoch": 1, "update": 0.864, "loss": "5.481", "nll_loss": "3.89", "ppl": "14.83", "wps": "3097.1", "ups": "7.29", "wpb": "424.6", "bsz": "32", "num_updates": "1500", "lr": "4.9975e-05", "gnorm": "4.045", "train_wall": "14", "wall": "193"}
2022-08-18 19:52:44 | INFO | train_inner | {"epoch": 1, "update": 0.922, "loss": "5.232", "nll_loss": "3.617", "ppl": "12.27", "wps": "3193.9", "ups": "8.29", "wpb": "385.5", "bsz": "32", "num_updates": "1600", "lr": "4.997e-05", "gnorm": "4.019", "train_wall": "12", "wall": "205"}
2022-08-18 19:52:57 | INFO | train_inner | {"epoch": 1, "update": 0.979, "loss": "5.315", "nll_loss": "3.71", "ppl": "13.08", "wps": "3250.2", "ups": "7.79", "wpb": "417.4", "bsz": "32", "num_updates": "1700", "lr": "4.9965e-05", "gnorm": "3.904", "train_wall": "13", "wall": "217"}
2022-08-18 19:53:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
/net/cephfs/scratch/jgu/ssr/fairseq/utils.py:342: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2022-08-18 19:56:22 | INFO | valid | {"epoch": 1, "valid_loss": "5.195", "valid_nll_loss": "3.524", "valid_ppl": "11.51", "valid_bleu": "9.24", "valid_wps": "1189.1", "valid_wpb": "411.1", "valid_bsz": "32", "valid_num_updates": "1736"}
2022-08-18 19:56:22 | INFO | fairseq_cli.train | begin save checkpoint
2022-08-18 19:56:28 | INFO | fairseq.checkpoint_utils | saved checkpoint /home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_python_en_XX/checkpoint_best.pt (epoch 1 @ 1736 updates, score 9.24) (writing took 6.021372936666012 seconds)
2022-08-18 19:56:28 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-08-18 19:56:28 | INFO | train | {"epoch": 1, "train_loss": "5.764", "train_nll_loss": "4.136", "train_ppl": "17.59", "train_wps": "1672.3", "train_ups": "4.08", "train_wpb": "410.2", "train_bsz": "32", "train_num_updates": "1736", "train_lr": "4.99632e-05", "train_gnorm": "6.347", "train_train_wall": "216", "train_wall": "428"}
2022-08-18 19:56:28 | INFO | fairseq.trainer | begin training epoch 2
/net/cephfs/scratch/jgu/ssr/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beams_buf = indices_buf // vocab_size
/net/cephfs/scratch/jgu/ssr/fairseq/sequence_generator.py:651: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  unfin_idx = idx // beam_size
2022-08-18 19:56:37 | INFO | train_inner | {"epoch": 2, "update": 1.037, "loss": "5.07", "nll_loss": "3.431", "ppl": "10.79", "wps": "177.8", "ups": "0.46", "wpb": "390", "bsz": "32", "num_updates": "1800", "lr": "4.996e-05", "gnorm": "3.962", "train_wall": "12", "wall": "437"}
2022-08-18 19:56:49 | INFO | train_inner | {"epoch": 2, "update": 1.094, "loss": "4.894", "nll_loss": "3.226", "ppl": "9.36", "wps": "3183.8", "ups": "8.2", "wpb": "388.2", "bsz": "32", "num_updates": "1900", "lr": "4.9955e-05", "gnorm": "3.967", "train_wall": "12", "wall": "449"}
2022-08-18 19:57:00 | INFO | train_inner | {"epoch": 2, "update": 1.152, "loss": "4.923", "nll_loss": "3.258", "ppl": "9.56", "wps": "3653.8", "ups": "8.9", "wpb": "410.7", "bsz": "32", "num_updates": "2000", "lr": "4.99499e-05", "gnorm": "3.888", "train_wall": "11", "wall": "460"}
2022-08-18 19:57:14 | INFO | train_inner | {"epoch": 2, "update": 1.21, "loss": "5.089", "nll_loss": "3.446", "ppl": "10.9", "wps": "3280", "ups": "7.36", "wpb": "445.4", "bsz": "32", "num_updates": "2100", "lr": "4.99449e-05", "gnorm": "3.92", "train_wall": "13", "wall": "474"}
2022-08-18 19:57:27 | INFO | train_inner | {"epoch": 2, "update": 1.267, "loss": "4.904", "nll_loss": "3.246", "ppl": "9.49", "wps": "3064.8", "ups": "7.68", "wpb": "399.3", "bsz": "31.9", "num_updates": "2200", "lr": "4.99399e-05", "gnorm": "3.946", "train_wall": "13", "wall": "487"}
2022-08-18 19:57:39 | INFO | train_inner | {"epoch": 2, "update": 1.325, "loss": "4.894", "nll_loss": "3.234", "ppl": "9.41", "wps": "3145.4", "ups": "7.93", "wpb": "396.6", "bsz": "32", "num_updates": "2300", "lr": "4.99349e-05", "gnorm": "3.935", "train_wall": "12", "wall": "499"}
2022-08-18 19:57:52 | INFO | train_inner | {"epoch": 2, "update": 1.382, "loss": "4.865", "nll_loss": "3.205", "ppl": "9.22", "wps": "3028.9", "ups": "7.74", "wpb": "391.5", "bsz": "32", "num_updates": "2400", "lr": "4.99299e-05", "gnorm": "3.976", "train_wall": "13", "wall": "512"}
2022-08-18 19:58:05 | INFO | train_inner | {"epoch": 2, "update": 1.44, "loss": "4.957", "nll_loss": "3.31", "ppl": "9.92", "wps": "3150.1", "ups": "7.73", "wpb": "407.6", "bsz": "32", "num_updates": "2500", "lr": "4.99249e-05", "gnorm": "3.911", "train_wall": "13", "wall": "525"}
2022-08-18 19:58:18 | INFO | train_inner | {"epoch": 2, "update": 1.498, "loss": "4.972", "nll_loss": "3.328", "ppl": "10.04", "wps": "3282.4", "ups": "7.95", "wpb": "412.9", "bsz": "32", "num_updates": "2600", "lr": "4.99199e-05", "gnorm": "3.912", "train_wall": "12", "wall": "538"}
2022-08-18 19:58:30 | INFO | train_inner | {"epoch": 2, "update": 1.555, "loss": "4.935", "nll_loss": "3.285", "ppl": "9.75", "wps": "3392.2", "ups": "7.95", "wpb": "426.9", "bsz": "32", "num_updates": "2700", "lr": "4.99149e-05", "gnorm": "3.867", "train_wall": "12", "wall": "550"}
2022-08-18 19:58:43 | INFO | train_inner | {"epoch": 2, "update": 1.613, "loss": "4.938", "nll_loss": "3.291", "ppl": "9.79", "wps": "3272.8", "ups": "7.81", "wpb": "419.2", "bsz": "32", "num_updates": "2800", "lr": "4.99099e-05", "gnorm": "3.874", "train_wall": "13", "wall": "563"}
2022-08-18 19:58:55 | INFO | train_inner | {"epoch": 2, "update": 1.671, "loss": "5.019", "nll_loss": "3.383", "ppl": "10.43", "wps": "3577.9", "ups": "8.1", "wpb": "441.8", "bsz": "32", "num_updates": "2900", "lr": "4.99049e-05", "gnorm": "3.847", "train_wall": "12", "wall": "576"}
2022-08-18 19:59:08 | INFO | train_inner | {"epoch": 2, "update": 1.728, "loss": "4.85", "nll_loss": "3.197", "ppl": "9.17", "wps": "3122.6", "ups": "7.85", "wpb": "397.8", "bsz": "32", "num_updates": "3000", "lr": "4.98999e-05", "gnorm": "3.982", "train_wall": "13", "wall": "588"}
2022-08-18 19:59:21 | INFO | train_inner | {"epoch": 2, "update": 1.786, "loss": "4.937", "nll_loss": "3.296", "ppl": "9.82", "wps": "3389.8", "ups": "7.99", "wpb": "424.3", "bsz": "32", "num_updates": "3100", "lr": "4.98949e-05", "gnorm": "3.857", "train_wall": "12", "wall": "601"}
2022-08-18 19:59:33 | INFO | train_inner | {"epoch": 2, "update": 1.843, "loss": "4.894", "nll_loss": "3.25", "ppl": "9.51", "wps": "3404.5", "ups": "8.25", "wpb": "412.5", "bsz": "32", "num_updates": "3200", "lr": "4.98899e-05", "gnorm": "3.906", "train_wall": "12", "wall": "613"}
2022-08-18 19:59:45 | INFO | train_inner | {"epoch": 2, "update": 1.901, "loss": "4.87", "nll_loss": "3.221", "ppl": "9.33", "wps": "3401.4", "ups": "8.33", "wpb": "408.6", "bsz": "32", "num_updates": "3300", "lr": "4.98849e-05", "gnorm": "3.874", "train_wall": "12", "wall": "625"}
2022-08-18 19:59:57 | INFO | train_inner | {"epoch": 2, "update": 1.959, "loss": "4.787", "nll_loss": "3.13", "ppl": "8.75", "wps": "3179.5", "ups": "7.94", "wpb": "400.2", "bsz": "32", "num_updates": "3400", "lr": "4.98799e-05", "gnorm": "3.91", "train_wall": "12", "wall": "638"}
2022-08-18 20:00:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 20:02:53 | INFO | valid | {"epoch": 2, "valid_loss": "5.03", "valid_nll_loss": "3.314", "valid_ppl": "9.95", "valid_bleu": "11.99", "valid_wps": "1434", "valid_wpb": "411.1", "valid_bsz": "32", "valid_num_updates": "3472", "valid_best_bleu": "11.99"}
2022-08-18 20:02:53 | INFO | fairseq_cli.train | begin save checkpoint
2022-08-18 20:03:00 | INFO | fairseq.checkpoint_utils | saved checkpoint /home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_python_en_XX/checkpoint_best.pt (epoch 2 @ 3472 updates, score 11.99) (writing took 6.7244017869234085 seconds)
2022-08-18 20:03:00 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-08-18 20:03:00 | INFO | train | {"epoch": 2, "train_loss": "4.917", "train_nll_loss": "3.265", "train_ppl": "9.61", "train_wps": "1816.2", "train_ups": "4.43", "train_wpb": "410.2", "train_bsz": "32", "train_num_updates": "3472", "train_lr": "4.98763e-05", "train_gnorm": "3.909", "train_train_wall": "215", "train_wall": "820"}
2022-08-18 20:03:00 | INFO | fairseq.trainer | begin training epoch 3
2022-08-18 20:03:04 | INFO | train_inner | {"epoch": 3, "update": 2.016, "loss": "4.707", "nll_loss": "3.033", "ppl": "8.19", "wps": "221", "ups": "0.53", "wpb": "413.5", "bsz": "32", "num_updates": "3500", "lr": "4.98749e-05", "gnorm": "3.827", "train_wall": "13", "wall": "825"}
2022-08-18 20:03:17 | INFO | train_inner | {"epoch": 3, "update": 2.074, "loss": "4.35", "nll_loss": "2.611", "ppl": "6.11", "wps": "3336.7", "ups": "8.14", "wpb": "409.9", "bsz": "32", "num_updates": "3600", "lr": "4.98699e-05", "gnorm": "3.913", "train_wall": "12", "wall": "837"}
2022-08-18 20:03:29 | INFO | train_inner | {"epoch": 3, "update": 2.131, "loss": "4.337", "nll_loss": "2.601", "ppl": "6.07", "wps": "3301.2", "ups": "8.08", "wpb": "408.6", "bsz": "32", "num_updates": "3700", "lr": "4.98649e-05", "gnorm": "3.918", "train_wall": "12", "wall": "849"}
2022-08-18 20:03:42 | INFO | train_inner | {"epoch": 3, "update": 2.189, "loss": "4.382", "nll_loss": "2.653", "ppl": "6.29", "wps": "3129.1", "ups": "7.59", "wpb": "412.1", "bsz": "32", "num_updates": "3800", "lr": "4.98599e-05", "gnorm": "3.976", "train_wall": "13", "wall": "862"}
2022-08-18 20:03:55 | INFO | train_inner | {"epoch": 3, "update": 2.247, "loss": "4.453", "nll_loss": "2.738", "ppl": "6.67", "wps": "3301.7", "ups": "7.99", "wpb": "413.4", "bsz": "32", "num_updates": "3900", "lr": "4.98549e-05", "gnorm": "4.007", "train_wall": "12", "wall": "875"}
2022-08-18 20:04:08 | INFO | train_inner | {"epoch": 3, "update": 2.304, "loss": "4.414", "nll_loss": "2.692", "ppl": "6.46", "wps": "3125.3", "ups": "7.88", "wpb": "396.8", "bsz": "32", "num_updates": "4000", "lr": "4.98498e-05", "gnorm": "4.105", "train_wall": "12", "wall": "888"}
2022-08-18 20:04:20 | INFO | train_inner | {"epoch": 3, "update": 2.362, "loss": "4.383", "nll_loss": "2.66", "ppl": "6.32", "wps": "3101.6", "ups": "7.73", "wpb": "401.3", "bsz": "32", "num_updates": "4100", "lr": "4.98448e-05", "gnorm": "4.015", "train_wall": "13", "wall": "901"}
2022-08-18 20:04:33 | INFO | train_inner | {"epoch": 3, "update": 2.419, "loss": "4.282", "nll_loss": "2.547", "ppl": "5.84", "wps": "3142.9", "ups": "8.24", "wpb": "381.3", "bsz": "32", "num_updates": "4200", "lr": "4.98398e-05", "gnorm": "4.109", "train_wall": "12", "wall": "913"}
2022-08-18 20:04:45 | INFO | train_inner | {"epoch": 3, "update": 2.477, "loss": "4.487", "nll_loss": "2.778", "ppl": "6.86", "wps": "3346.3", "ups": "7.95", "wpb": "421", "bsz": "32", "num_updates": "4300", "lr": "4.98348e-05", "gnorm": "4.067", "train_wall": "12", "wall": "925"}
2022-08-18 20:04:57 | INFO | train_inner | {"epoch": 3, "update": 2.535, "loss": "4.512", "nll_loss": "2.81", "ppl": "7.01", "wps": "3441.2", "ups": "8.36", "wpb": "411.8", "bsz": "32", "num_updates": "4400", "lr": "4.98298e-05", "gnorm": "4.158", "train_wall": "12", "wall": "937"}
2022-08-18 20:05:09 | INFO | train_inner | {"epoch": 3, "update": 2.592, "loss": "4.364", "nll_loss": "2.644", "ppl": "6.25", "wps": "3473.4", "ups": "8.73", "wpb": "397.9", "bsz": "32", "num_updates": "4500", "lr": "4.98248e-05", "gnorm": "4.056", "train_wall": "11", "wall": "949"}
2022-08-18 20:05:21 | INFO | train_inner | {"epoch": 3, "update": 2.65, "loss": "4.436", "nll_loss": "2.726", "ppl": "6.62", "wps": "3184.4", "ups": "7.81", "wpb": "407.9", "bsz": "31.9", "num_updates": "4600", "lr": "4.98198e-05", "gnorm": "4.145", "train_wall": "13", "wall": "962"}
2022-08-18 20:05:34 | INFO | train_inner | {"epoch": 3, "update": 2.707, "loss": "4.396", "nll_loss": "2.684", "ppl": "6.43", "wps": "3236.6", "ups": "8.17", "wpb": "396.1", "bsz": "32", "num_updates": "4700", "lr": "4.98148e-05", "gnorm": "4.141", "train_wall": "12", "wall": "974"}
2022-08-18 20:05:46 | INFO | train_inner | {"epoch": 3, "update": 2.765, "loss": "4.453", "nll_loss": "2.746", "ppl": "6.71", "wps": "3412.2", "ups": "8.15", "wpb": "418.4", "bsz": "32", "num_updates": "4800", "lr": "4.98098e-05", "gnorm": "4.108", "train_wall": "12", "wall": "986"}
2022-08-18 20:05:59 | INFO | train_inner | {"epoch": 3, "update": 2.823, "loss": "4.518", "nll_loss": "2.822", "ppl": "7.07", "wps": "3321.9", "ups": "7.82", "wpb": "424.7", "bsz": "32", "num_updates": "4900", "lr": "4.98048e-05", "gnorm": "4.064", "train_wall": "13", "wall": "999"}
2022-08-18 20:06:13 | INFO | train_inner | {"epoch": 3, "update": 2.88, "loss": "4.452", "nll_loss": "2.748", "ppl": "6.72", "wps": "2987", "ups": "7.12", "wpb": "419.2", "bsz": "32", "num_updates": "5000", "lr": "4.97998e-05", "gnorm": "4.108", "train_wall": "14", "wall": "1013"}
2022-08-18 20:06:26 | INFO | train_inner | {"epoch": 3, "update": 2.938, "loss": "4.585", "nll_loss": "2.897", "ppl": "7.45", "wps": "3339.1", "ups": "7.53", "wpb": "443.6", "bsz": "32", "num_updates": "5100", "lr": "4.97948e-05", "gnorm": "4.157", "train_wall": "13", "wall": "1026"}
2022-08-18 20:06:38 | INFO | train_inner | {"epoch": 3, "update": 2.995, "loss": "4.476", "nll_loss": "2.776", "ppl": "6.85", "wps": "3330.6", "ups": "8.05", "wpb": "413.8", "bsz": "32", "num_updates": "5200", "lr": "4.97898e-05", "gnorm": "4.121", "train_wall": "12", "wall": "1039"}
2022-08-18 20:06:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 20:09:26 | INFO | valid | {"epoch": 3, "valid_loss": "4.961", "valid_nll_loss": "3.249", "valid_ppl": "9.5", "valid_bleu": "13.68", "valid_wps": "1435.4", "valid_wpb": "411.1", "valid_bsz": "32", "valid_num_updates": "5208", "valid_best_bleu": "13.68"}
2022-08-18 20:09:26 | INFO | fairseq_cli.train | begin save checkpoint
2022-08-18 20:09:33 | INFO | fairseq.checkpoint_utils | saved checkpoint /home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_python_en_XX/checkpoint_best.pt (epoch 3 @ 5208 updates, score 13.68) (writing took 6.9200203865766525 seconds)
2022-08-18 20:09:33 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-08-18 20:09:33 | INFO | train | {"epoch": 3, "train_loss": "4.429", "train_nll_loss": "2.715", "train_ppl": "6.57", "train_wps": "1812.9", "train_ups": "4.42", "train_wpb": "410.2", "train_bsz": "32", "train_num_updates": "5208", "train_lr": "4.97894e-05", "train_gnorm": "4.066", "train_train_wall": "215", "train_wall": "1213"}
2022-08-18 20:09:33 | INFO | fairseq.trainer | begin training epoch 4
2022-08-18 20:09:46 | INFO | train_inner | {"epoch": 4, "update": 3.053, "loss": "3.994", "nll_loss": "2.209", "ppl": "4.62", "wps": "218.6", "ups": "0.53", "wpb": "409.1", "bsz": "32", "num_updates": "5300", "lr": "4.97848e-05", "gnorm": "3.998", "train_wall": "13", "wall": "1226"}
2022-08-18 20:09:59 | INFO | train_inner | {"epoch": 4, "update": 3.111, "loss": "3.888", "nll_loss": "2.087", "ppl": "4.25", "wps": "3024.3", "ups": "7.71", "wpb": "392.2", "bsz": "32", "num_updates": "5400", "lr": "4.97798e-05", "gnorm": "4.143", "train_wall": "13", "wall": "1239"}
2022-08-18 20:10:11 | INFO | train_inner | {"epoch": 4, "update": 3.168, "loss": "4.044", "nll_loss": "2.265", "ppl": "4.81", "wps": "3633.6", "ups": "8.36", "wpb": "434.9", "bsz": "32", "num_updates": "5500", "lr": "4.97748e-05", "gnorm": "4.112", "train_wall": "12", "wall": "1251"}
2022-08-18 20:10:24 | INFO | train_inner | {"epoch": 4, "update": 3.226, "loss": "4.032", "nll_loss": "2.254", "ppl": "4.77", "wps": "3188.5", "ups": "7.7", "wpb": "414.3", "bsz": "32", "num_updates": "5600", "lr": "4.97698e-05", "gnorm": "4.279", "train_wall": "13", "wall": "1264"}
2022-08-18 20:10:37 | INFO | train_inner | {"epoch": 4, "update": 3.283, "loss": "4.061", "nll_loss": "2.284", "ppl": "4.87", "wps": "3184", "ups": "7.54", "wpb": "422.3", "bsz": "32", "num_updates": "5700", "lr": "4.97648e-05", "gnorm": "4.261", "train_wall": "13", "wall": "1277"}
2022-08-18 20:10:49 | INFO | train_inner | {"epoch": 4, "update": 3.341, "loss": "3.961", "nll_loss": "2.175", "ppl": "4.51", "wps": "3151.5", "ups": "7.95", "wpb": "396.5", "bsz": "32", "num_updates": "5800", "lr": "4.97598e-05", "gnorm": "4.289", "train_wall": "12", "wall": "1290"}
2022-08-18 20:11:01 | INFO | train_inner | {"epoch": 4, "update": 3.399, "loss": "3.962", "nll_loss": "2.177", "ppl": "4.52", "wps": "3475.8", "ups": "8.55", "wpb": "406.4", "bsz": "32", "num_updates": "5900", "lr": "4.97548e-05", "gnorm": "4.305", "train_wall": "12", "wall": "1301"}
2022-08-18 20:11:13 | INFO | train_inner | {"epoch": 4, "update": 3.456, "loss": "3.952", "nll_loss": "2.169", "ppl": "4.5", "wps": "3349.4", "ups": "8.44", "wpb": "396.9", "bsz": "32", "num_updates": "6000", "lr": "4.97497e-05", "gnorm": "4.29", "train_wall": "12", "wall": "1313"}
2022-08-18 20:11:26 | INFO | train_inner | {"epoch": 4, "update": 3.514, "loss": "3.972", "nll_loss": "2.195", "ppl": "4.58", "wps": "3071.2", "ups": "7.92", "wpb": "387.6", "bsz": "32", "num_updates": "6100", "lr": "4.97447e-05", "gnorm": "4.39", "train_wall": "12", "wall": "1326"}
2022-08-18 20:11:38 | INFO | train_inner | {"epoch": 4, "update": 3.571, "loss": "4.146", "nll_loss": "2.389", "ppl": "5.24", "wps": "3515.6", "ups": "8.26", "wpb": "425.8", "bsz": "32", "num_updates": "6200", "lr": "4.97397e-05", "gnorm": "4.31", "train_wall": "12", "wall": "1338"}
2022-08-18 20:11:51 | INFO | train_inner | {"epoch": 4, "update": 3.629, "loss": "4.113", "nll_loss": "2.352", "ppl": "5.11", "wps": "3155.5", "ups": "7.51", "wpb": "420.2", "bsz": "32", "num_updates": "6300", "lr": "4.97347e-05", "gnorm": "4.342", "train_wall": "13", "wall": "1351"}
2022-08-18 20:12:05 | INFO | train_inner | {"epoch": 4, "update": 3.687, "loss": "4.144", "nll_loss": "2.387", "ppl": "5.23", "wps": "3100.5", "ups": "7.13", "wpb": "435", "bsz": "32", "num_updates": "6400", "lr": "4.97297e-05", "gnorm": "4.35", "train_wall": "14", "wall": "1365"}
2022-08-18 20:12:18 | INFO | train_inner | {"epoch": 4, "update": 3.744, "loss": "4.014", "nll_loss": "2.242", "ppl": "4.73", "wps": "3178.5", "ups": "7.76", "wpb": "409.7", "bsz": "32", "num_updates": "6500", "lr": "4.97247e-05", "gnorm": "4.311", "train_wall": "13", "wall": "1378"}
2022-08-18 20:12:30 | INFO | train_inner | {"epoch": 4, "update": 3.802, "loss": "3.959", "nll_loss": "2.184", "ppl": "4.54", "wps": "3275.7", "ups": "8.42", "wpb": "389.2", "bsz": "31.9", "num_updates": "6600", "lr": "4.97197e-05", "gnorm": "4.441", "train_wall": "12", "wall": "1390"}
2022-08-18 20:12:42 | INFO | train_inner | {"epoch": 4, "update": 3.859, "loss": "4.079", "nll_loss": "2.321", "ppl": "5", "wps": "3316.1", "ups": "8.06", "wpb": "411.3", "bsz": "32", "num_updates": "6700", "lr": "4.97147e-05", "gnorm": "4.42", "train_wall": "12", "wall": "1402"}
2022-08-18 20:12:55 | INFO | train_inner | {"epoch": 4, "update": 3.917, "loss": "4.076", "nll_loss": "2.317", "ppl": "4.98", "wps": "3243.8", "ups": "7.84", "wpb": "413.7", "bsz": "32", "num_updates": "6800", "lr": "4.97097e-05", "gnorm": "4.399", "train_wall": "13", "wall": "1415"}
2022-08-18 20:13:08 | INFO | train_inner | {"epoch": 4, "update": 3.975, "loss": "4.088", "nll_loss": "2.334", "ppl": "5.04", "wps": "3234.8", "ups": "7.82", "wpb": "413.5", "bsz": "32", "num_updates": "6900", "lr": "4.97047e-05", "gnorm": "4.375", "train_wall": "13", "wall": "1428"}
2022-08-18 20:13:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-08-18 20:16:08 | INFO | valid | {"epoch": 4, "valid_loss": "5.013", "valid_nll_loss": "3.285", "valid_ppl": "9.75", "valid_bleu": "15.75", "valid_wps": "1368.6", "valid_wpb": "411.1", "valid_bsz": "32", "valid_num_updates": "6944", "valid_best_bleu": "15.75"}
2022-08-18 20:16:08 | INFO | fairseq_cli.train | begin save checkpoint
2022-08-18 20:16:14 | INFO | fairseq.checkpoint_utils | saved checkpoint /home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_python_en_XX/checkpoint_best.pt (epoch 4 @ 6944 updates, score 15.75) (writing took 6.058342166244984 seconds)
2022-08-18 20:16:14 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-08-18 20:16:14 | INFO | train | {"epoch": 4, "train_loss": "4.028", "train_nll_loss": "2.255", "train_ppl": "4.77", "train_wps": "1776.9", "train_ups": "4.33", "train_wpb": "410.2", "train_bsz": "32", "train_num_updates": "6944", "train_lr": "4.97025e-05", "train_gnorm": "4.297", "train_train_wall": "216", "train_wall": "1614"}
2022-08-18 20:16:14 | INFO | fairseq_cli.train | done training in 1611.9 seconds
@ Completed
@ Stage 3
Create Datastore
2022-08-18 20:16:16 | INFO | fairseq_cli.validate | loading model(s) from /home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_python_en_XX/checkpoint_best.pt
2022-08-18 20:16:18 | INFO | fairseq.tasks.translation | [python] dictionary: 50001 types
2022-08-18 20:16:18 | INFO | fairseq.tasks.translation | [en_XX] dictionary: 50001 types
2022-08-18 20:16:25 | INFO | fairseq_cli.validate | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='mbart_base', attention_dropout=0.0, batch_size=32, batch_size_valid=32, best_checkpoint_metric='bleu', bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/home/cluster/jgu/scratch/ssr/cli/out/mix/python/data-bin', data_buffer_size=10, dataset_impl='mmap', ddp_backend='no_c10d', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, dstore_filename=None, dstore_fp16=True, dstore_mmap='/home/cluster/jgu/scratch/ssr/cli/out/mix/python/8th_datastore', dstore_size=601026, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eval_bleu=True, eval_bleu_args='{"beam": 6}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, k=5, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, knn_lambda=0.5, knn_sim_metric=None, knn_temperature=10, label_smoothing=0.1, langs='java,python,en_XX', layernorm_embedding=True, left_pad_source=False, left_pad_target=False, load_alignments=False, localsgd_frequency=3, log_format='json', log_interval=100, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=4, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, move_dstore_to_mem=False, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, only_train_final_output=False, optimizer='adam', optimizer_overrides='{}', partially_finetune=False, path='/home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_python_en_XX/checkpoint_best.pt', patience=5, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, prepend_bos=False, probe=32, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='/home/cluster/jgu/scratch/ssr/cli/model/plbart_base.pt', save_dir='/home/cluster/jgu/scratch/ssr/cli/out/mix/8th_base_python_en_XX', save_interval=1, save_interval_updates=0, scoring='bleu', seed=42, sentence_avg=False, sentencepiece_model='/home/cluster/jgu/scratch/ssr/cli/sentencepiece/sentencepiece.bpe.model', shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='python', stop_time_hours=0, target_lang='en_XX', task='translation_from_pretrained_bart', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_gpu_to_search=False, use_knn_datastore=False, use_old_adam=False, user_dir='/home/cluster/jgu/scratch/ssr/cli', valid_subset='train', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=1000, weight_decay=0.0, zero_sharding='none')
Saving fp16
2022-08-18 20:16:25 | INFO | fairseq.data.data_utils | loaded 55538 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/python/data-bin/train.python-en_XX.python
2022-08-18 20:16:25 | INFO | fairseq.data.data_utils | loaded 55538 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/python/data-bin/train.python-en_XX.en_XX
2022-08-18 20:16:25 | INFO | fairseq.tasks.translation | /home/cluster/jgu/scratch/ssr/cli/out/mix/python/data-bin train python-en_XX 55538 examples
/home/cluster/jgu/scratch/ssr/cli/ds_build.py:69: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dstore_vals = np.memmap(args.dstore_mmap + '/vals.npy', dtype=np.int, mode='w+',
/home/cluster/jgu/scratch/ssr/cli/ds_build.py:157: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dstore_vals[dstore_idx:reduce_size + dstore_idx] = target.unsqueeze(-1).cpu().numpy().astype(np.int)
1376
2939
3541
5902
6393
9091
10412
10999
13774
15437
18306
20298
20887
23807
26148
26605
29326
32183
33513
36086
39168
40421
42623
45760
48170
48884
51006
53977
56292
56947
58920
61581
64697
66030
67665
69959
72934
74635
76332
78602
81525
83264
83953
85604
87772
90564
92601
93233
94771
96770
99183
101997
103220
104620
106474
108760
111402
112237
113571
115292
117357
119919
121736
122862
124420
126337
128678
130435
131235
132627
134368
136462
139196
140197
141476
143030
144872
147155
148759
149875
151335
153113
155326
156975
157528
158601
159937
161511
163379
165820
167045
168226
169672
171407
173534
175239
176218
177459
178909
180591
182806
183807
184855
186173
187699
189524
191509
192202
193213
194451
195908
197652
199610
200355
201436
202771
204347
206289
207701
208248
209147
210260
211577
213145
215136
216095
217002
218118
219423
220972
222910
223772
224536
225516
226662
227992
229536
231589
232589
233484
234558
235796
237239
238960
240794
241481
242420
243551
244866
246445
248085
248759
249571
250572
251719
253071
254768
256034
256784
257702
258791
260044
261535
263236
264030
264855
265839
266974
268294
269980
271789
272411
273228
274183
275285
276576
278196
279352
280018
280849
281800
282898
284200
285952
286652
287429
288333
289358
290525
291868
293710
294528
295266
296169
297188
298359
299738
301385
301922
302619
303457
304437
305551
306836
308253
308803
309473
310283
311221
312296
313598
315358
315911
316631
317485
318480
319630
321048
322466
323069
323842
324747
325796
327004
328595
329452
330161
331000
331983
333128
334529
335696
336253
336929
337707
338602
339657
340965
342366
342920
343596
344375
345269
346300
347579
348677
349236
349956
350804
351808
353041
354305
354812
355478
356260
357171
358235
359806
360451
361103
361882
362802
363887
365319
366272
366768
367365
368046
368842
369753
370832
371942
372445
373058
373753
374557
375512
376759
377730
378248
378863
379578
380377
381334
382683
383196
383784
384460
385249
386153
387309
388237
388780
389443
390191
391076
392112
393456
393940
394558
395282
396123
397157
398551
399020
399623
400343
401167
402161
403639
404168
404770
405495
406365
407462
408463
408899
409441
410051
410733
411524
412484
413828
414231
414740
415318
415989
416792
417812
418661
419135
419711
420373
421120
421975
423079
423970
424450
425037
425747
426584
427586
428611
429060
429617
430256
430977
431810
432898
433565
434043
434609
435247
435979
436849
438125
438519
439014
439598
440271
441043
441947
443147
443578
444119
444757
445493
446386
447705
448100
448608
449189
449853
450640
451592
452719
453079
453517
454018
454568
455199
455910
456776
457452
457850
458329
458884
459520
460262
461276
461950
462347
462836
463391
464035
464802
465911
466240
466672
467205
467821
468549
469463
470080
470464
470925
471464
472076
472803
473736
474262
474671
475153
475718
476370
477133
478262
478698
479136
479653
480268
481012
482077
482551
483004
483536
484143
484851
485838
486439
486844
487321
487876
488523
489291
490336
490671
491121
491655
492291
493031
494143
494483
494934
495476
496106
496864
497953
498309
498780
499342
500004
500828
501628
501936
502313
502745
503240
503808
504473
505287
505578
505941
506355
506831
507381
508028
508954
509339
509684
510092
510570
511125
511823
512696
513002
513385
513844
514384
515005
515790
516522
516834
517226
517688
518230
518885
519629
519917
520286
520734
521253
521863
522725
523065
523443
523891
524410
525055
525928
526202
526547
526961
527457
528018
528705
529321
529648
530053
530528
531094
531899
532395
532742
533149
533617
534162
534806
535560
535878
536278
536742
537267
537937
538902
539193
539586
540045
540575
541243
541886
542226
542637
543121
543664
544322
544898
545228
545637
546114
546682
547376
548072
548407
548834
549350
549962
550728
551311
551660
552084
552599
553233
554207
554561
554930
555390
555949
556665
557146
557435
557766
558142
558583
559115
559790
560039
560371
560752
561195
561706
562354
562864
563138
563480
563877
564331
564884
565330
565594
565912
566289
566745
567284
567970
568207
568508
568848
569234
569678
570213
570971
571185
571460
571809
572210
572689
573376
573770
574054
574404
574816
575311
576001
576338
576614
576946
577341
577823
578467
578973
579235
579558
579943
580384
580948
581728
581950
582241
582606
583025
583523
584231
584460
584792
585181
585626
586198
586803
587058
587381
587758
588206
588797
589318
589598
589960
590386
590916
591706
591955
592271
592648
593085
593616
594144
594425
594781
595203
595739
596457
596682
596977
597336
597762
598312
598828
599092
599408
599776
600197
600707
601026
much more than dstore size break
Build Faiss Index
/home/cluster/jgu/scratch/ssr/cli/ds_train.py:28: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  vals = np.memmap(args.dstore_mmap + '/vals.npy', dtype=np.int, mode='r', shape=(args.dstore_size, 1))
Namespace(code_size=64, dimension=768, dstore_fp16=True, dstore_mmap='/home/cluster/jgu/scratch/ssr/cli/out/mix/python/8th_datastore', dstore_size=601026, faiss_index='/home/cluster/jgu/scratch/ssr/cli/out/mix/python/8th_datastore/knn_index', ncentroids=4096, probe=32, seed=1, starting_point=0)
done.
Start put index to gpu
Training Index
[372585  13403 243731 423153 392567 512461 516163 453239  41018 558783]
[[-4.72   -0.5728  0.981  ...  0.2512  1.116  -0.7695]
 [-2.244  -0.7666  2.799  ... -4.61   -0.7397  2.7   ]
 [ 1.051  -0.344   0.199  ... -0.235   0.3127 -0.396 ]
 ...
 [-0.5835  0.3867 -0.2537 ...  0.0646 -0.2307  1.766 ]
 [-1.473   1.666   1.422  ... -0.7847 -0.463  -2.271 ]
 [-2.797  -1.241  -0.3337 ...  0.5366 -0.6846  1.094 ]]
Training took 20.049927711486816 s
Writing index after training
Writing index took 0.03369855880737305 s
Adding Keys
Added 1000000 tokens so far
Writing Index 1000000
Adding total 601026 keys
Adding took 4.312021255493164 s
Writing Index
Writing index took 0.15127253532409668 s
@ Completed
@ Stage 4
/net/cephfs/scratch/jgu/ssr/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beams_buf = indices_buf // vocab_size
/net/cephfs/scratch/jgu/ssr/fairseq/sequence_generator.py:651: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  unfin_idx = idx // beam_size
WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.
c_bleu = 15.62 | s_bleu = 28.23 | meteor = 18.36 | rouge = 39.91
@ Completed

@ Stage 3
Build Datastore
2022-08-29 09:10:28 | INFO | fairseq_cli.validate | loading model(s) from /home/cluster/jgu/scratch/ssr/cli/out/mix/half_base_java_en_XX/checkpoint_best.pt
2022-08-29 09:10:29 | INFO | fairseq.tasks.translation | [java] dictionary: 50001 types
2022-08-29 09:10:29 | INFO | fairseq.tasks.translation | [en_XX] dictionary: 50001 types
2022-08-29 09:10:36 | INFO | fairseq_cli.validate | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='mbart_base', attention_dropout=0.0, batch_size=32, batch_size_valid=32, best_checkpoint_metric='bleu', bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/home/cluster/jgu/scratch/ssr/cli/out/csn/java/data-bin', data_buffer_size=10, dataset_impl='mmap', ddp_backend='no_c10d', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, dstore_filename=None, dstore_fp16=True, dstore_mmap='/home/cluster/jgu/scratch/ssr/cli/out/mix/java/half_shared_datastore', dstore_size=2387231, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eval_bleu=True, eval_bleu_args='{"beam": 6}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, k=5, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, knn_lambda=0.5, knn_sim_metric=None, knn_temperature=10, label_smoothing=0.1, langs='java,python,en_XX', layernorm_embedding=True, left_pad_source=False, left_pad_target=False, load_alignments=False, localsgd_frequency=3, log_format='json', log_interval=100, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=21, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, move_dstore_to_mem=False, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, only_train_final_output=False, optimizer='adam', optimizer_overrides='{}', partially_finetune=False, path='/home/cluster/jgu/scratch/ssr/cli/out/mix/half_base_java_en_XX/checkpoint_best.pt', patience=5, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, prepend_bos=False, probe=32, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='/home/cluster/jgu/scratch/ssr/cli/model/plbart_base.pt', save_dir='/home/cluster/jgu/scratch/ssr/cli/out/mix/half_base_java_en_XX', save_interval=1, save_interval_updates=0, scoring='bleu', seed=42, sentence_avg=False, sentencepiece_model='/home/cluster/jgu/scratch/ssr/cli/sentencepiece/sentencepiece.bpe.model', shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='java', stop_time_hours=0, target_lang='en_XX', task='translation_from_pretrained_bart', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_gpu_to_search=False, use_knn_datastore=False, use_old_adam=False, user_dir='/home/cluster/jgu/scratch/ssr/cli', valid_subset='train', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=1000, weight_decay=0.0, zero_sharding='none')
Saving fp16
2022-08-29 09:10:36 | INFO | fairseq.data.data_utils | loaded 164923 examples from: /home/cluster/jgu/scratch/ssr/cli/out/csn/java/data-bin/train.java-en_XX.java
2022-08-29 09:10:36 | INFO | fairseq.data.data_utils | loaded 164923 examples from: /home/cluster/jgu/scratch/ssr/cli/out/csn/java/data-bin/train.java-en_XX.en_XX
2022-08-29 09:10:36 | INFO | fairseq.tasks.translation | /home/cluster/jgu/scratch/ssr/cli/out/csn/java/data-bin train java-en_XX 164923 examples
/home/cluster/jgu/scratch/ssr/cli/ds_build.py:69: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dstore_vals = np.memmap(args.dstore_mmap + '/vals.npy', dtype=np.int, mode='w+',
/home/cluster/jgu/scratch/ssr/cli/ds_build.py:157: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dstore_vals[dstore_idx:reduce_size + dstore_idx] = target.unsqueeze(-1).cpu().numpy().astype(np.int)
850
2135
3590
4230
6201
7423
8810
11037
11779
13043
14952
17271
18092
19251
20737
22642
25411
26839
27866
29166
30699
32537
34989
37225
37836
38733
39887
41238
42748
44495
46655
49486
50704
51510
52589
53865
55315
56945
58808
61076
63919
65426
66150
67074
68156
69351
70677
72139
73727
75501
77580
80338
82752
83121
83932
84921
86044
87290
88673
90180
91817
93638
95853
98607
99928
100646
101346
102220
103217
104314
105506
106816
108247
109788
111479
113410
115798
118658
119857
120102
120760
121565
122468
123478
124551
125695
126937
128280
129719
131276
132997
134950
137282
140439
142132
142678
143338
144139
145073
146090
147203
148379
149657
151011
152486
154092
155891
157904
160411
163361
165415
165741
166355
167085
167922
168866
169884
170963
172115
173349
174659
176075
177603
179276
181126
183291
186114
188411
189172
189775
190499
191321
192257
193245
194301
195419
196610
197876
199232
200682
202265
204058
206153
208879
210992
211501
212105
212840
213681
214613
215595
216651
217770
218964
220227
221590
223045
224614
226338
228272
230625
233831
235929
236452
237091
237819
238616
239496
240400
241368
242375
243431
244572
245756
247002
248348
249811
251420
253213
255237
257690
260727
262202
262447
263012
263665
264392
265196
266076
267000
267968
269014
270115
271273
272517
273845
275268
276819
278535
280520
282914
285847
287223
287576
288131
288769
289488
290298
291178
292091
293059
294060
295116
296257
297439
298687
300030
301484
303070
304819
306834
309305
311739
312881
313405
314026
314736
315528
316393
317302
318270
319278
320334
321467
322654
323922
325268
326715
328275
330002
331974
334251
337248
338994
339495
340058
340702
341422
342190
343016
343896
344818
345778
346813
347893
349033
350249
351559
352968
354534
356346
358551
361648
362978
363453
364013
364643
365345
366095
366895
367770
368685
369645
370667
371740
372876
374076
375352
376738
378276
380031
382066
384554
386653
387322
387821
388401
389052
389772
390553
391369
392249
393157
394117
395136
396199
397325
398525
399800
401167
402671
404334
406280
408692
411255
411965
412483
413075
413735
414455
415255
416086
416970
417930
418909
419953
421073
422241
423468
424782
426206
427755
429513
431539
434028
436469
437189
437684
438267
438911
439631
440415
441237
442117
443030
443990
445022
446122
447287
448541
449872
451289
452835
454551
456523
458907
461584
462314
462731
463233
463798
464411
465082
465802
466564
467364
468228
469112
470048
471040
472072
473152
474300
475530
476857
478305
479944
481778
483942
486697
487302
487775
488316
488925
489590
490310
491064
491856
492678
493542
494465
495431
496441
497521
498665
499901
501197
502587
504142
505949
508141
510784
512333
512776
513280
513850
514493
515185
515905
516690
517486
518350
519247
520183
521186
522223
523319
524477
525737
527065
528512
530167
532058
534278
536627
537387
537847
538375
538958
539606
540305
541044
541836
542669
543533
544434
545379
546387
547455
548597
549824
551156
552601
554248
556150
558517
560561
560995
561518
562099
562747
563449
564169
564958
565773
566637
567549
568496
569504
570576
571716
572941
574271
575722
577354
579263
581790
583122
583536
584003
584515
585084
585682
586322
586991
587695
588435
589203
590022
590861
591757
592695
593684
594739
595844
597054
598369
599805
601402
603194
605516
607663
608344
608750
609214
609726
610297
610897
611537
612234
612952
613720
614501
615333
616200
617096
618050
619046
620110
621244
622453
623708
625076
626606
628414
630809
632844
633232
633680
634191
634731
635307
635942
636582
637285
638022
638790
639596
640428
641300
642196
643125
644118
645170
646282
647474
648746
650142
651748
653671
656205
657316
657718
658172
658684
659259
659880
660520
661202
661906
662662
663444
664276
665145
666041
666997
668006
669072
670217
671472
672907
674578
676511
678763
679761
680161
680620
681132
681705
682314
682954
683633
684337
685099
685871
686703
687564
688460
689417
690428
691500
692647
693909
695298
696864
698693
701070
702353
702722
703165
703669
704214
704815
705455
706120
706824
707539
708307
709122
709957
710853
711769
712729
713753
714849
716038
717340
718777
720384
722196
724404
726682
727039
727487
727996
728557
729158
729798
730493
731197
731960
732728
733541
734373
735263
736206
737225
738328
739486
740737
742039
743455
745059
746988
749453
750421
750839
751313
751829
752405
753024
753678
754382
755126
755894
756723
757594
758504
759464
760466
761530
762691
763936
765272
766734
768445
770636
772778
773097
773480
773904
774358
774862
775400
775960
776571
777187
777857
778533
779261
780024
780826
781672
782568
783514
784529
785617
786805
788137
789647
791534
793477
794217
794564
794956
795403
795898
796425
796985
797582
798198
798854
799535
800263
801003
801787
802599
803472
804371
805329
806370
807546
808892
810485
812473
814064
814343
814702
815111
815567
816072
816632
817211
817827
818474
819146
819864
820606
821390
822203
823046
823947
824899
825921
827031
828279
829719
831411
833922
834517
834870
835268
835716
836201
836723
837283
837864
838480
839129
839801
840515
841251
842035
842853
843719
844649
845665
846755
847914
849167
850524
852092
854090
855782
856105
856504
856958
857462
858005
858576
859192
859852
860529
861257
862024
862831
863691
864587
865512
866496
867533
868633
869831
871179
872713
874562
876607
876930
877313
877761
878264
878796
879356
879958
880581
881253
881928
882656
883395
884179
885004
885867
886785
887752
888805
889979
891271
892770
894636
896844
897169
897561
898006
898484
898992
899552
900134
900750
901396
902079
902807
903569
904374
905218
906121
907098
908172
909383
910757
912332
914441
915594
915941
916347
916809
917313
917856
918427
919043
919690
920362
921083
921833
922628
923468
924362
925289
926281
927345
928544
929915
931499
933482
935586
935902
936301
936749
937248
937797
938367
938983
939611
940283
940997
941725
942508
943312
944165
945077
946061
947135
948307
949624
951128
952942
955477
956211
956536
956900
957300
957732
958193
958673
959187
959715
960280
960856
961476
962113
962785
963490
964232
965033
965859
966734
967672
968684
969793
971028
972517
974519
975356
975672
976027
976414
976846
977312
977792
978306
978834
979408
979989
980613
981267
981946
982666
983422
984220
985064
985975
986957
988060
989303
990767
992543
994013
994326
994680
995075
995507
995969
996450
996978
997506
998077
998654
999278
999912
1000584
1001302
1002054
1002872
1003755
1004712
1005750
1006937
1008305
1009956
1011886
1012167
1012503
1012887
1013306
1013747
1014227
1014707
1015233
1015768
1016344
1016947
1017589
1018261
1018970
1019716
1020519
1021352
1022256
1023263
1024391
1025698
1027249
1029458
1030056
1030389
1030767
1031179
1031615
1032095
1032600
1033128
1033695
1034279
1034903
1035567
1036254
1036993
1037764
1038594
1039475
1040413
1041422
1042560
1043896
1045592
1047457
1047759
1048117
1048501
1048933
1049385
1049865
1050388
1050933
1051509
1052109
1052733
1053401
1054099
1054840
1055636
1056493
1057410
1058408
1059520
1060760
1062215
1064152
1064840
1065164
1065533
1065938
1066375
1066855
1067362
1067890
1068455
1069051
1069675
1070336
1071037
1071782
1072573
1073412
1074355
1075416
1076644
1078167
1080345
1081048
1081356
1081706
1082090
1082520
1082973
1083458
1083986
1084530
1085106
1085723
1086356
1087028
1087736
1088491
1089298
1090145
1091078
1092128
1093334
1094791
1096850
1097510
1097841
1098216
1098637
1099089
1099569
1100086
1100614
1101190
1101784
1102408
1103078
1103781
1104526
1105318
1106157
1107058
1108056
1109201
1110558
1112324
1113671
1113975
1114328
1114713
1115145
1115623
1116131
1116659
1117233
1117844
1118468
1119131
1119815
1120535
1121296
1122107
1122967
1123883
1124881
1126014
1127382
1129060
1130776
1131083
1131444
1131839
1132271
1132736
1133217
1133745
1134303
1134889
1135513
1136184
1136886
1137633
1138437
1139318
1140247
1141293
1142486
1143913
1145845
1146844
1147153
1147502
1147900
1148333
1148813
1149315
1149843
1150403
1150983
1151607
1152277
1152986
1153729
1154526
1155386
1156332
1157398
1158632
1160125
1162065
1163217
1163477
1163773
1164098
1164458
1164842
1165242
1165663
1166103
1166561
1167041
1167536
1168061
1168621
1169218
1169866
1170552
1171287
1172066
1172937
1173898
1175013
1176333
1178053
1178926
1179194
1179503
1179831
1180191
1180566
1180966
1181392
1181832
1182307
1182796
1183316
1183860
1184425
1185025
1185660
1186329
1187049
1187808
1188628
1189532
1190530
1191677
1193084
1194494
1194728
1195008
1195315
1195643
1196003
1196388
1196788
1197210
1197650
1198122
1198602
1199121
1199654
1200220
1200822
1201471
1202165
1202910
1203695
1204571
1205550
1206705
1208092
1209984
1210218
1210498
1210810
1211140
1211500
1211883
1212283
1212716
1213168
1213648
1214160
1214687
1215247
1215845
1216461
1217110
1217792
1218523
1219306
1220162
1221114
1222217
1223523
1224937
1225590
1225856
1226157
1226478
1226838
1227227
1227637
1228077
1228539
1229019
1229519
1230039
1230593
1231178
1231796
1232479
1233223
1233983
1234785
1235678
1236660
1237740
1239037
1240840
1241603
1241856
1242149
1242486
1242846
1243228
1243628
1244052
1244492
1244971
1245479
1246008
1246568
1247155
1247777
1248443
1249151
1249925
1250777
1251730
1252833
1254200
1255454
1255836
1256090
1256373
1256693
1257046
1257413
1257813
1258246
1258686
1259156
1259651
1260171
1260721
1261281
1261877
1262487
1263141
1263843
1264597
1265430
1266372
1267470
1268803
1270380
1271693
1271947
1272246
1272577
1272943
1273343
1273772
1274212
1274692
1275181
1275701
1276254
1276832
1277441
1278092
1278803
1279556
1280362
1281234
1282204
1283344
1284829
1286444
1286702
1287002
1287346
1287728
1288128
1288563
1289022
1289502
1290019
1290553
1291125
1291733
1292382
1293074
1293818
1294621
1295523
1296576
1297788
1299314
1300298
1300559
1300871
1301222
1301595
1301995
1302432
1302872
1303350
1303846
1304366
1304921
1305510
1306128
1306789
1307508
1308287
1309087
1309912
1310842
1311903
1313198
1315236
1315515
1315783
1316081
1316413
1316779
1317179
1317595
1318035
1318503
1318988
1319508
1320046
1320614
1321223
1321865
1322552
1323299
1324127
1325047
1326133
1327475
1329156
1329410
1329709
1330048
1330434
1330834
1331274
1331729
1332209
1332729
1333274
1333856
1334478
1335147
1335859
1336668
1337588
1338632
1339853
1341412
1342637
1342876
1343170
1343490
1343850
1344243
1344651
1345091
1345556
1346036
1346554
1347096
1347660
1348266
1348910
1349604
1350374
1351205
1352154
1353303
1354749
1356458
1356696
1356979
1357299
1357658
1358049
1358460
1358900
1359377
1359866
1360386
1360938
1361529
1362161
1362851
1363602
1364438
1365425
1366593
1368083
1369427
1369683
1369978
1370311
1370684
1371084
1371521
1371977
1372459
1372979
1373542
1374151
1374797
1375495
1376250
1377086
1377926
1378826
1379852
1381088
1382771
1383421
1383689
1383992
1384322
1384683
1385083
1385509
1385953
1386433
1386948
1387483
1388058
1388676
1389332
1390032
1390798
1391650
1392629
1393813
1395568
1395986
1396261
1396581
1396940
1397330
1397739
1398179
1398657
1399161
1399701
1400274
1400894
1401562
1402299
1403126
1404054
1405109
1406319
1407969
1408758
1408959
1409193
1409458
1409746
1410057
1410388
1410740
1411100
1411484
1411877
1412296
1412744
1413219
1413715
1414238
1414791
1415386
1416033
1416743
1417545
1418472
1419647
1421023
1421224
1421456
1421712
1421998
1422298
1422618
1422956
1423308
1423687
1424075
1424491
1424937
1425397
1425887
1426403
1426947
1427523
1428146
1428813
1429542
1430363
1431299
1432427
1433954
1434201
1434421
1434662
1434919
1435207
1435498
1435818
1436141
1436493
1436852
1437236
1437636
1438061
1438509
1438994
1439519
1440078
1440692
1441370
1442074
1442799
1443601
1444486
1445590
1446986
1447178
1447402
1447652
1447920
1448208
1448519
1448846
1449198
1449558
1449942
1450346
1450762
1451209
1451666
1452148
1452667
1453226
1453842
1454512
1455270
1456111
1457098
1458415
1459502
1459712
1459942
1460198
1460484
1460786
1461106
1461457
1461811
1462195
1462592
1463008
1463450
1463908
1464402
1464931
1465505
1466118
1466791
1467539
1468451
1469562
1470996
1471190
1471426
1471682
1471965
1472253
1472564
1472884
1473232
1473584
1473967
1474369
1474788
1475236
1475714
1476217
1476753
1477346
1477994
1478691
1479448
1480297
1481353
1482975
1483159
1483383
1483634
1483910
1484204
1484524
1484856
1485208
1485586
1485973
1486389
1486834
1487304
1487805
1488349
1488936
1489577
1490285
1491082
1492009
1493137
1494725
1495130
1495354
1495591
1495853
1496141
1496461
1496795
1497147
1497521
1497914
1498330
1498777
1499245
1499745
1500279
1500846
1501459
1502133
1502867
1503688
1504655
1505946
1506911
1507128
1507378
1507654
1507949
1508269
1508613
1508965
1509347
1509737
1510153
1510593
1511052
1511541
1512059
1512609
1513187
1513805
1514466
1515188
1516002
1517021
1518524
1518956
1519180
1519428
1519685
1519973
1520280
1520605
1520957
1521313
1521697
1522107
1522548
1523010
1523490
1523999
1524544
1525140
1525800
1526554
1527475
1528573
1529928
1530115
1530330
1530554
1530804
1531069
1531362
1531682
1532034
1532399
1532787
1533203
1533638
1534096
1534585
1535113
1535681
1536302
1536970
1537707
1538548
1539564
1540839
1541872
1542086
1542322
1542578
1542865
1543162
1543482
1543831
1544195
1544579
1544992
1545429
1545901
1546398
1546924
1547483
1548088
1548745
1549482
1550341
1551415
1553144
1553535
1553756
1554005
1554274
1554574
1554899
1555251
1555625
1556021
1556437
1556871
1557326
1557810
1558333
1558903
1559545
1560264
1561080
1562121
1563763
1563958
1564193
1564449
1564734
1565046
1565378
1565731
1566115
1566504
1566920
1567368
1567853
1568376
1568934
1569531
1570186
1570927
1571790
1572858
1574574
1574756
1574985
1575241
1575517
1575815
1576135
1576483
1576847
1577231
1577645
1578089
1578566
1579072
1579616
1580202
1580849
1581571
1582388
1583330
1584498
1585937
1586132
1586360
1586616
1586904
1587194
1587514
1587857
1588215
1588599
1589004
1589436
1589907
1590408
1590944
1591533
1592181
1592924
1593785
1594893
1596431
1596623
1596847
1597101
1597377
1597669
1597989
1598320
1598672
1599054
1599450
1599874
1600329
1600818
1601337
1601895
1602494
1603153
1603897
1604782
1605895
1607634
1607829
1608067
1608323
1608610
1608920
1609251
1609603
1609981
1610370
1610786
1611226
1611692
1612181
1612710
1613280
1613906
1614609
1615437
1616470
1617942
1618624
1618856
1619114
1619402
1619718
1620060
1620419
1620803
1621213
1621652
1622120
1622622
1623156
1623730
1624348
1625033
1625836
1626773
1627896
1629214
1629419
1629663
1629937
1630236
1630556
1630902
1631278
1631686
1632121
1632571
1633059
1633577
1634135
1634750
1635436
1636244
1637214
1638605
1639338
1639557
1639812
1640098
1640406
1640750
1641124
1641521
1641937
1642377
1642848
1643351
1643882
1644453
1645087
1645827
1646695
1647750
1649387
1649573
1649798
1650054
1650337
1650647
1650988
1651350
1651738
1652154
1652597
1653057
1653564
1654123
1654740
1655442
1656297
1657363
1658802
1659591
1659799
1660045
1660328
1660638
1660970
1661322
1661706
1662117
1662549
1663009
1663514
1664072
1664680
1665360
1666122
1667042
1668267
1669159
1669378
1669625
1669903
1670207
1670534
1670886
1671267
1671675
1672096
1672547
1673035
1673567
1674171
1674850
1675645
1676611
1677891
1678765
1678986
1679242
1679530
1679832
1680156
1680519
1680909
1681325
1681772
1682242
1682748
1683296
1683895
1684575
1685378
1686367
1687896
1688287
1688517
1688779
1689071
1689391
1689734
1690112
1690508
1690928
1691376
1691858
1692379
1692943
1693554
1694239
1695020
1695935
1697182
1698078
1698230
1698409
1698606
1698824
1699064
1699321
1699590
1699878
1700186
1700502
1700838
1701195
1701573
1701976
1702424
1702912
1703449
1704047
1704735
1705618
1706682
1706830
1707008
1707208
1707426
1707666
1707925
1708203
1708494
1708806
1709129
1709476
1709846
1710239
1710647
1711077
1711534
1712025
1712583
1713204
1713905
1714718
1715609
1715766
1715942
1716134
1716349
1716584
1716826
1717090
1717373
1717669
1717981
1718298
1718634
1718994
1719379
1719787
1720226
1720701
1721244
1721865
1722594
1723511
1724363
1724518
1724694
1724886
1725101
1725324
1725564
1725826
1726094
1726382
1726690
1727018
1727369
1727737
1728130
1728551
1729008
1729525
1730109
1730775
1731547
1732603
1733130
1733294
1733485
1733697
1733928
1734182
1734446
1734734
1735036
1735359
1735706
1736073
1736468
1736888
1737339
1737832
1738368
1738986
1739715
1740633
1741682
1741827
1741997
1742189
1742398
1742614
1742847
1743093
1743357
1743621
1743908
1744209
1744534
1744882
1745257
1745657
1746093
1746573
1747122
1747734
1748417
1749253
1750507
1750909
1751079
1751271
1751486
1751711
1751956
1752220
1752490
1752778
1753084
1753408
1753753
1754128
1754533
1754974
1755458
1756001
1756638
1757456
1758712
1759156
1759316
1759496
1759691
1759907
1760143
1760383
1760646
1760923
1761222
1761539
1761879
1762245
1762638
1763073
1763558
1764094
1764705
1765406
1766259
1767495
1767893
1768068
1768267
1768485
1768725
1768989
1769260
1769548
1769859
1770188
1770540
1770913
1771317
1771758
1772239
1772775
1773389
1774100
1774920
1775962
1776501
1776663
1776846
1777046
1777269
1777509
1777761
1778025
1778313
1778622
1778947
1779292
1779655
1780039
1780448
1780897
1781392
1781949
1782600
1783398
1784603
1784826
1785000
1785201
1785417
1785652
1785896
1786160
1786447
1786749
1787067
1787403
1787755
1788132
1788541
1788984
1789466
1790003
1790626
1791418
1792515
1792787
1792968
1793177
1793408
1793650
1793914
1794190
1794490
1794811
1795147
1795502
1795887
1796308
1796776
1797300
1797928
1798717
1799762
1800425
1800583
1800773
1800985
1801225
1801488
1801759
1802047
1802354
1802679
1803024
1803393
1803786
1804201
1804649
1805142
1805688
1806299
1807074
1808127
1808713
1808867
1809044
1809246
1809462
1809702
1809957
1810225
1810515
1810827
1811149
1811500
1811868
1812270
1812712
1813206
1813761
1814409
1815217
1816441
1816692
1816866
1817066
1817291
1817538
1817802
1818082
1818385
1818697
1819032
1819391
1819766
1820163
1820597
1821068
1821605
1822235
1822974
1823846
1825130
1825268
1825434
1825628
1825844
1826082
1826331
1826597
1826885
1827195
1827521
1827867
1828248
1828666
1829128
1829642
1830211
1830880
1831673
1832749
1833308
1833467
1833656
1833860
1834089
1834343
1834608
1834897
1835209
1835534
1835879
1836244
1836633
1837058
1837530
1838062
1838681
1839424
1840481
1841109
1841276
1841468
1841681
1841920
1842174
1842448
1842743
1843062
1843402
1843768
1844169
1844609
1845081
1845611
1846201
1846916
1847821
1848519
1848679
1848866
1849076
1849306
1849560
1849829
1850122
1850435
1850776
1851143
1851528
1851946
1852401
1852937
1853584
1854430
1855220
1855384
1855576
1855772
1855993
1856239
1856503
1856790
1857092
1857410
1857746
1858099
1858478
1858892
1859336
1859829
1860381
1861067
1861914
1863012
1863168
1863358
1863575
1863815
1864074
1864348
1864641
1864960
1865309
1865686
1866099
1866573
1867103
1867741
1868576
1869475
1869636
1869828
1870044
1870282
1870535
1870808
1871096
1871401
1871735
1872100
1872491
1872920
1873406
1873961
1874622
1875584
1876296
1876461
1876662
1876895
1877151
1877431
1877739
1878070
1878428
1878810
1879223
1879680
1880182
1880767
1881476
1882368
1883220
1883373
1883553
1883764
1883991
1884239
1884506
1884794
1885105
1885443
1885813
1886210
1886647
1887138
1887704
1888389
1889485
1889700
1889871
1890068
1890288
1890528
1890791
1891076
1891383
1891713
1892069
1892453
1892866
1893309
1893802
1894372
1895075
1896034
1896843
1897001
1897190
1897401
1897627
1897867
1898131
1898421
1898736
1899080
1899450
1899852
1900304
1900815
1901409
1902128
1903172
1903607
1903785
1903991
1904225
1904489
1904772
1905077
1905409
1905766
1906149
1906578
1907066
1907651
1908383
1909412
1909943
1910113
1910314
1910542
1910793
1911067
1911355
1911668
1912004
1912370
1912765
1913206
1913686
1914208
1914816
1915560
1916576
1916943
1917112
1917321
1917551
1917795
1918060
1918348
1918657
1918995
1919366
1919766
1920217
1920742
1921402
1922318
1923448
1923610
1923804
1924031
1924271
1924538
1924826
1925136
1925475
1925848
1926267
1926738
1927296
1927928
1928710
1929812
1929966
1930149
1930356
1930585
1930827
1931091
1931378
1931680
1932010
1932360
1932748
1933181
1933666
1934234
1934907
1935826
1936300
1936471
1936666
1936889
1937129
1937392
1937680
1937986
1938315
1938667
1939054
1939476
1939969
1940558
1941285
1942471
1942612
1942796
1943007
1943250
1943514
1943799
1944116
1944460
1944832
1945244
1945726
1946290
1946952
1947859
1948798
1948966
1949161
1949378
1949622
1949897
1950203
1950534
1950894
1951297
1951760
1952311
1952977
1953929
1954411
1954590
1954804
1955049
1955314
1955603
1955916
1956256
1956627
1957042
1957528
1958098
1958796
1959655
1960560
1960716
1960913
1961142
1961388
1961664
1961968
1962300
1962660
1963057
1963500
1964009
1964674
1965638
1965836
1966019
1966233
1966478
1966743
1967031
1967341
1967677
1968044
1968441
1968889
1969397
1970027
1970872
1971752
1971915
1972108
1972335
1972586
1972853
1973141
1973453
1973782
1974137
1974543
1975014
1975562
1976268
1977057
1977220
1977420
1977662
1977926
1978212
1978521
1978869
1979248
1979647
1980094
1980608
1981237
1982088
1983327
1983472
1983669
1983896
1984141
1984408
1984696
1985005
1985340
1985699
1986094
1986529
1987007
1987578
1988270
1989268
1989872
1990059
1990279
1990534
1990813
1991116
1991453
1991820
1992215
1992670
1993198
1993850
1994849
1995055
1995232
1995437
1995669
1995933
1996216
1996525
1996868
1997266
1997711
1998213
1998824
1999628
2000249
2000352
2000470
2000598
2000740
2000895
2001059
2001235
2001427
2001630
2001849
2002084
2002337
2002604
2002899
2003217
2003568
2003948
2004406
2004955
2005701
2005913
2006022
2006145
2006277
2006421
2006578
2006740
2006916
2007098
2007290
2007492
2007702
2007927
2008176
2008442
2008724
2009033
2009380
2009776
2010228
2010823
2011462
2011569
2011691
2011830
2011979
2012140
2012317
2012509
2012717
2012937
2013172
2013423
2013693
2013999
2014338
2014728
2015211
2015846
2016538
2016637
2016755
2016894
2017054
2017218
2017394
2017584
2017783
2017996
2018220
2018458
2018710
2018986
2019293
2019637
2020012
2020455
2020971
2021660
2021759
2021876
2022015
2022169
2022332
2022510
2022702
2022908
2023126
2023355
2023601
2023871
2024163
2024479
2024837
2025251
2025786
2026486
2026662
2026782
2026916
2027066
2027226
2027402
2027585
2027785
2028000
2028228
2028476
2028751
2029051
2029380
2029751
2030195
2030841
2031157
2031274
2031405
2031549
2031705
2031876
2032059
2032253
2032466
2032695
2032939
2033200
2033485
2033793
2034137
2034517
2034951
2035454
2036197
2036615
2036727
2036862
2037012
2037172
2037343
2037527
2037719
2037928
2038152
2038387
2038642
2038918
2039222
2039569
2039956
2040414
2041174
2041338
2041458
2041592
2041742
2041906
2042083
2042277
2042487
2042711
2042953
2043216
2043500
2043808
2044150
2044544
2044993
2045544
2046247
2046777
2046884
2047012
2047147
2047297
2047457
2047631
2047821
2048021
2048238
2048471
2048722
2048998
2049294
2049617
2049999
2050433
2050976
2051879
2052063
2052181
2052315
2052460
2052621
2052797
2052981
2053176
2053389
2053617
2053867
2054143
2054442
2054773
2055143
2055570
2056094
2056777
2056871
2056993
2057129
2057279
2057439
2057611
2057798
2058004
2058224
2058466
2058730
2059009
2059320
2059669
2060088
2060571
2061216
2061834
2061931
2062052
2062187
2062333
2062497
2062677
2062869
2063073
2063290
2063521
2063780
2064062
2064360
2064689
2065061
2065504
2066089
2066844
2066950
2067075
2067220
2067380
2067549
2067738
2067950
2068183
2068433
2068711
2069012
2069350
2069726
2070166
2070681
2071365
2071469
2071592
2071734
2071891
2072056
2072232
2072423
2072621
2072832
2073061
2073309
2073577
2073872
2074196
2074574
2075028
2075597
2076268
2076367
2076496
2076646
2076812
2076992
2077184
2077388
2077609
2077845
2078111
2078404
2078717
2079054
2079435
2079880
2080432
2081143
2081675
2081779
2081903
2082049
2082214
2082391
2082583
2082787
2083001
2083235
2083488
2083761
2084064
2084417
2084853
2085428
2086227
2086323
2086450
2086594
2086751
2086922
2087105
2087300
2087508
2087730
2087972
2088246
2088555
2088914
2089332
2089824
2090536
2090800
2090922
2091062
2091219
2091394
2091585
2091796
2092026
2092277
2092548
2092845
2093176
2093566
2094026
2094626
2095015
2095137
2095288
2095453
2095629
2095821
2096030
2096254
2096490
2096748
2097032
2097352
2097709
2098107
2098598
2099232
2099667
2099781
2099921
2100081
2100254
2100436
2100637
2100850
2101078
2101323
2101586
2101869
2102176
2102511
2102894
2103347
2103896
2104730
2104913
2105032
2105172
2105331
2105498
2105679
2105874
2106083
2106307
2106551
2106820
2107118
2107450
2107866
2108378
2109054
2109495
2109602
2109734
2109882
2110047
2110223
2110411
2110612
2110827
2111060
2111312
2111591
2111901
2112248
2112648
2113149
2113781
2114077
2114199
2114335
2114497
2114673
2114857
2115053
2115266
2115496
2115747
2116033
2116344
2116689
2117088
2117581
2118272
2118365
2118481
2118616
2118765
2118925
2119097
2119285
2119486
2119711
2119951
2120215
2120503
2120812
2121163
2121580
2122113
2122767
2122868
2122983
2123114
2123258
2123415
2123580
2123766
2123966
2124181
2124417
2124676
2124965
2125308
2125712
2126189
2126804
2127518
2127612
2127729
2127864
2128019
2128187
2128372
2128574
2128803
2129050
2129314
2129607
2129932
2130320
2130817
2131448
2132178
2132281
2132400
2132533
2132686
2132849
2133025
2133214
2133418
2133635
2133865
2134118
2134400
2134713
2135071
2135490
2136009
2136712
2136815
2136940
2137090
2137253
2137429
2137609
2137804
2138012
2138234
2138471
2138732
2139026
2139353
2139714
2140151
2140718
2141335
2141442
2141575
2141729
2141889
2142065
2142247
2142447
2142662
2142890
2143139
2143416
2143717
2144058
2144524
2145216
2145508
2145621
2145753
2145907
2146079
2146264
2146468
2146702
2146974
2147290
2147643
2148078
2148643
2149245
2149366
2149508
2149670
2149851
2150051
2150262
2150489
2150738
2151016
2151338
2151725
2152252
2153059
2153225
2153345
2153493
2153665
2153854
2154064
2154288
2154527
2154799
2155114
2155469
2155869
2156338
2156939
2157266
2157383
2157519
2157671
2157848
2158044
2158257
2158492
2158741
2159014
2159314
2159667
2160092
2160656
2161243
2161349
2161479
2161629
2161803
2161995
2162203
2162427
2162674
2162943
2163246
2163589
2163987
2164488
2165171
2165326
2165447
2165588
2165746
2165922
2166109
2166319
2166555
2166818
2167106
2167427
2167788
2168225
2168811
2169272
2169376
2169505
2169656
2169822
2170010
2170221
2170449
2170694
2170956
2171265
2171639
2172119
2172750
2173239
2173353
2173503
2173669
2173848
2174057
2174288
2174545
2174822
2175134
2175494
2175935
2176553
2176803
2176928
2177070
2177230
2177405
2177593
2177799
2178026
2178278
2178554
2178867
2179246
2179722
2180391
2180495
2180622
2180772
2180941
2181133
2181347
2181573
2181820
2182100
2182413
2182776
2183189
2183720
2184088
2184195
2184332
2184486
2184660
2184852
2185055
2185277
2185518
2185781
2186065
2186377
2186741
2187173
2187710
2188226
2188337
2188473
2188624
2188792
2188976
2189187
2189417
2189682
2189985
2190327
2190708
2191168
2191952
2192160
2192291
2192440
2192613
2192811
2193025
2193259
2193526
2193825
2194179
2194587
2195058
2195794
2196088
2196201
2196338
2196493
2196664
2196854
2197062
2197289
2197537
2197815
2198132
2198523
2198993
2199503
2199615
2199756
2199924
2200106
2200305
2200516
2200746
2200994
2201273
2201586
2201941
2202392
2203020
2203479
2203596
2203734
2203894
2204064
2204248
2204452
2204676
2204915
2205181
2205480
2205824
2206254
2206826
2206975
2207097
2207253
2207426
2207624
2207832
2208057
2208306
2208582
2208891
2209257
2209745
2210194
2210312
2210454
2210619
2210802
2211005
2211225
2211475
2211743
2212052
2212408
2212844
2213500
2213933
2214055
2214207
2214378
2214568
2214777
2215013
2215271
2215565
2215911
2216336
2216888
2217290
2217401
2217544
2217722
2217926
2218151
2218399
2218679
2219023
2219451
2220004
2220818
2220919
2221053
2221202
2221373
2221564
2221764
2221985
2222234
2222521
2222863
2223277
2223823
2224100
2224221
2224361
2224519
2224698
2224894
2225111
2225342
2225597
2225885
2226225
2226634
2227232
2227482
2227591
2227738
2227906
2228095
2228301
2228537
2228805
2229124
2229486
2229935
2230516
2230867
2230989
2231135
2231305
2231495
2231716
2231963
2232230
2232534
2232911
2233426
2234179
2234285
2234421
2234578
2234761
2234961
2235181
2235422
2235699
2236050
2236507
2237186
2237552
2237676
2237826
2238004
2238204
2238430
2238691
2239018
2239449
2240001
2240529
2240645
2240783
2240948
2241128
2241340
2241586
2241853
2242171
2242652
2243422
2243524
2243656
2243806
2243981
2244172
2244378
2244604
2244868
2245175
2245557
2246078
2246493
2246601
2246733
2246882
2247053
2247254
2247471
2247722
2248009
2248352
2248766
2249385
2249874
2249988
2250118
2250270
2250455
2250669
2250902
2251170
2251480
2251867
2252513
2253004
2253121
2253266
2253429
2253613
2253814
2254040
2254302
2254646
2255261
2255409
2255531
2255681
2255860
2256059
2256278
2256532
2256815
2257135
2257572
2258247
2258476
2258604
2258759
2258940
2259140
2259364
2259622
2259943
2260342
2260910
2261472
2261603
2261772
2261959
2262172
2262408
2262676
2262994
2263385
2263849
2264584
2264796
2264926
2265082
2265261
2265456
2265667
2265899
2266169
2266474
2266841
2267322
2268140
2268257
2268408
2268588
2268787
2269027
2269293
2269596
2269963
2270471
2271083
2271201
2271343
2271505
2271689
2271891
2272108
2272349
2272624
2272980
2273456
2273987
2274103
2274256
2274434
2274636
2274857
2275107
2275412
2275789
2276258
2276800
2276933
2277107
2277302
2277516
2277754
2278024
2278341
2278753
2279322
2279778
2279898
2280054
2280237
2280442
2280665
2280921
2281210
2281539
2281952
2282615
2282833
2282977
2283142
2283321
2283521
2283752
2284022
2284351
2284798
2285464
2285771
2285891
2286032
2286199
2286391
2286604
2286844
2287108
2287405
2287771
2288286
2288542
2288677
2288838
2289028
2289238
2289479
2289765
2290126
2290625
2291498
2291595
2291729
2291893
2292079
2292280
2292498
2292740
2293036
2293401
2293899
2294396
2294516
2294684
2294876
2295095
2295344
2295629
2295986
2296497
2296798
2296922
2297086
2297270
2297468
2297696
2297971
2298346
2298915
2299201
2299330
2299490
2299672
2299873
2300124
2300434
2300795
2301264
2301971
2302148
2302282
2302447
2302632
2302850
2303108
2303444
2303880
2304602
2304705
2304846
2305015
2305204
2305417
2305663
2305955
2306328
2306819
2307467
2307623
2307749
2307900
2308072
2308285
2308536
2308824
2309171
2309623
2310330
2310451
2310601
2310775
2310970
2311200
2311460
2311760
2312150
2312796
2312998
2313144
2313317
2313511
2313725
2313996
2314337
2314781
2315456
2315562
2315691
2315855
2316043
2316249
2316487
2316759
2317090
2317500
2318112
2318396
2318517
2318657
2318820
2319020
2319245
2319509
2319832
2320258
2320922
2321119
2321258
2321422
2321609
2321826
2322077
2322370
2322731
2323205
2323863
2323980
2324134
2324319
2324534
2324786
2325106
2325541
2325886
2325931
2325989
2326057
2326131
2326212
2326300
2326393
2326493
2326597
2326712
2326838
2326973
2327119
2327291
2327496
2327724
2328019
2328335
2328375
2328415
2328455
2328495
2328535
2328575
2328615
2328655
2328695
2328735
2328775
2328815
2328855
2328895
2328935
2328975
2329015
2329055
2329095
2329135
2329175
2329215
2329255
2329295
2329335
2329375
2329415
2329455
2329495
2329535
2329575
2329615
2329655
2329695
2329735
2329775
2329815
2329855
2329895
2329935
2329975
2330015
2330055
2330095
2330136
2330184
2330232
2330280
2330328
2330376
2330424
2330472
2330520
2330568
2330616
2330664
2330712
2330760
2330808
2330856
2330904
2330952
2331000
2331048
2331096
2331144
2331192
2331240
2331288
2331336
2331384
2331432
2331480
2331528
2331576
2331624
2331672
2331720
2331768
2331816
2331864
2331912
2331960
2332008
2332056
2332104
2332152
2332200
2332248
2332296
2332344
2332392
2332440
2332488
2332536
2332584
2332632
2332680
2332728
2332776
2332824
2332872
2332920
2332968
2333016
2333064
2333112
2333160
2333208
2333256
2333304
2333352
2333400
2333448
2333496
2333544
2333592
2333640
2333688
2333736
2333784
2333832
2333880
2333928
2333976
2334024
2334072
2334120
2334168
2334216
2334264
2334312
2334360
2334408
2334456
2334504
2334552
2334600
2334648
2334696
2334752
2334808
2334864
2334920
2334976
2335032
2335088
2335144
2335200
2335256
2335312
2335368
2335424
2335480
2335536
2335592
2335648
2335704
2335760
2335816
2335872
2335928
2335984
2336040
2336096
2336152
2336208
2336264
2336320
2336376
2336432
2336488
2336544
2336600
2336656
2336712
2336768
2336824
2336880
2336936
2336992
2337048
2337104
2337160
2337216
2337272
2337328
2337384
2337440
2337496
2337552
2337608
2337664
2337720
2337776
2337832
2337888
2337944
2338000
2338056
2338112
2338168
2338224
2338280
2338336
2338392
2338448
2338504
2338560
2338616
2338672
2338728
2338784
2338840
2338896
2338952
2339008
2339064
2339120
2339176
2339232
2339288
2339344
2339400
2339456
2339512
2339568
2339624
2339680
2339736
2339792
2339848
2339904
2339960
2340016
2340072
2340128
2340184
2340240
2340296
2340352
2340408
2340464
2340520
2340576
2340632
2340688
2340744
2340800
2340856
2340912
2340968
2341024
2341080
2341136
2341192
2341248
2341304
2341360
2341416
2341472
2341528
2341584
2341640
2341696
2341752
2341808
2341864
2341920
2341976
2342032
2342088
2342149
2342213
2342277
2342341
2342405
2342469
2342533
2342597
2342661
2342725
2342789
2342853
2342917
2342981
2343045
2343109
2343173
2343237
2343301
2343365
2343429
2343493
2343557
2343621
2343685
2343749
2343813
2343877
2343941
2344005
2344069
2344133
2344197
2344261
2344325
2344389
2344453
2344517
2344581
2344645
2344709
2344773
2344837
2344901
2344965
2345029
2345093
2345157
2345221
2345285
2345349
2345413
2345477
2345541
2345605
2345669
2345733
2345797
2345861
2345925
2345989
2346053
2346117
2346181
2346245
2346309
2346373
2346437
2346501
2346565
2346629
2346693
2346757
2346821
2346885
2346949
2347013
2347077
2347141
2347205
2347269
2347333
2347397
2347461
2347525
2347589
2347653
2347717
2347781
2347845
2347909
2347973
2348037
2348101
2348165
2348229
2348293
2348357
2348421
2348485
2348549
2348613
2348677
2348741
2348805
2348869
2348933
2348997
2349061
2349125
2349189
2349253
2349317
2349381
2349445
2349509
2349573
2349637
2349701
2349765
2349829
2349893
2349957
2350021
2350085
2350149
2350213
2350277
2350341
2350405
2350469
2350533
2350597
2350661
2350725
2350789
2350853
2350917
2350981
2351045
2351109
2351173
2351237
2351301
2351365
2351434
2351506
2351578
2351650
2351722
2351794
2351866
2351938
2352010
2352082
2352154
2352226
2352298
2352370
2352442
2352514
2352586
2352658
2352730
2352802
2352874
2352946
2353018
2353090
2353162
2353234
2353306
2353378
2353450
2353522
2353594
2353666
2353738
2353810
2353882
2353954
2354026
2354098
2354170
2354242
2354314
2354386
2354458
2354530
2354602
2354674
2354746
2354818
2354890
2354962
2355034
2355106
2355178
2355250
2355322
2355394
2355466
2355538
2355610
2355682
2355754
2355826
2355898
2355970
2356042
2356114
2356186
2356258
2356330
2356402
2356474
2356546
2356618
2356690
2356762
2356834
2356906
2356978
2357050
2357122
2357194
2357266
2357338
2357410
2357482
2357554
2357626
2357698
2357770
2357842
2357914
2357986
2358058
2358130
2358202
2358274
2358346
2358418
2358490
2358562
2358634
2358706
2358778
2358850
2358922
2358994
2359066
2359138
2359210
2359282
2359354
2359426
2359498
2359570
2359642
2359714
2359786
2359858
2359930
2360002
2360074
2360146
2360218
2360290
2360362
2360434
2360506
2360578
2360650
2360722
2360794
2360866
2360938
2361010
2361082
2361154
2361226
2361298
2361370
2361442
2361514
2361586
2361658
2361730
2361802
2361874
2361946
2362018
2362090
2362162
2362234
2362306
2362378
2362450
2362522
2362594
2362666
2362738
2362810
2362882
2362954
2363026
2363098
2363170
2363242
2363319
2363399
2363479
2363559
2363639
2363719
2363799
2363879
2363959
2364039
2364119
2364199
2364279
2364359
2364439
2364519
2364599
2364679
2364759
2364839
2364919
2364999
2365079
2365159
2365239
2365319
2365399
2365479
2365559
2365639
2365719
2365799
2365879
2365959
2366039
2366119
2366199
2366279
2366359
2366439
2366519
2366599
2366679
2366759
2366839
2366919
2366999
2367079
2367159
2367239
2367319
2367399
2367479
2367559
2367639
2367719
2367799
2367879
2367959
2368039
2368119
2368199
2368279
2368359
2368439
2368519
2368599
2368679
2368759
2368839
2368919
2368999
2369079
2369159
2369239
2369319
2369399
2369479
2369559
2369639
2369719
2369799
2369879
2369959
2370039
2370119
2370199
2370279
2370359
2370439
2370519
2370599
2370679
2370759
2370839
2370919
2370999
2371079
2371159
2371239
2371319
2371399
2371479
2371559
2371639
2371719
2371799
2371879
2371959
2372039
2372119
2372199
2372279
2372359
2372439
2372519
2372599
2372679
2372759
2372839
2372919
2372999
2373079
2373159
2373239
2373319
2373399
2373479
2373559
2373639
2373719
2373799
2373879
2373959
2374039
2374119
2374199
2374279
2374359
2374439
2374519
2374599
2374679
2374759
2374839
2374919
2374999
2375079
2375159
2375239
2375319
2375399
2375479
2375559
2375639
2375719
2375799
2375879
2375959
2376039
2376119
2376199
2376279
2376359
2376439
2376519
2376599
2376679
2376759
2376839
2376919
2376999
2377079
2377159
2377239
2377319
2377399
2377479
2377559
2377646
2377734
2377822
2377910
2377998
2378086
2378174
2378262
2378350
2378438
2378526
2378614
2378702
2378790
2378878
2378966
2379054
2379142
2379230
2379318
2379406
2379494
2379582
2379670
2379758
2379846
2379934
2380022
2380110
2380198
2380286
2380374
2380462
2380550
2380638
2380726
2380814
2380902
2380990
2381078
2381166
2381254
2381342
2381430
2381518
2381606
2381694
2381782
2381870
2381958
2382046
2382134
2382222
2382310
2382398
2382486
2382574
2382662
2382750
2382838
2382926
2383014
2383102
2383190
2383278
2383366
2383454
2383542
2383630
2383718
2383806
2383894
2383982
2384070
2384158
2384246
2384334
2384422
2384510
2384598
2384686
2384774
2384862
2384950
2385038
2385126
2385214
2385302
2385390
2385478
2385566
2385654
2385742
2385830
2385918
2386006
2386094
2386182
2386270
2386358
2386446
2386534
2386622
2386710
2386798
2386886
2386974
2387062
2387150
2387231
much more than dstore size break
Build Faiss Index
/home/cluster/jgu/scratch/ssr/cli/ds_train.py:28: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  vals = np.memmap(args.dstore_mmap + '/vals.npy', dtype=np.int, mode='r', shape=(args.dstore_size, 1))
Namespace(code_size=64, dimension=768, dstore_fp16=True, dstore_mmap='/home/cluster/jgu/scratch/ssr/cli/out/mix/java/half_shared_datastore', dstore_size=2387231, faiss_index='/home/cluster/jgu/scratch/ssr/cli/out/mix/java/half_shared_datastore/knn_index', ncentroids=4096, probe=32, seed=1, starting_point=0)
done.
Start put index to gpu
Training Index
[2196900 1353803 1400917  544762  929690   35632  572716  837770 1600494
  950636]
[[ 6.855    4.37    -4.332   ... -3.373   -1.628    1.624  ]
 [ 1.047    0.9746  -1.227   ... -1.412   -0.524    1.047  ]
 [-0.4524  -0.585    0.4448  ... -2.58     0.831   -0.6514 ]
 ...
 [-0.1675  -1.164   -1.227   ... -1.724   -0.4631  -0.4949 ]
 [-3.385    0.708    1.242   ... -3.402   -1.796   -0.6035 ]
 [-0.4858  -0.07916  1.43    ... -0.6255  -0.1329   1.136  ]]
Training took 37.66159152984619 s
Writing index after training
Writing index took 0.017435789108276367 s
Adding Keys
Added 1000000 tokens so far
Writing Index 1000000
Added 2000000 tokens so far
Writing Index 2000000
Adding total 2387231 keys
Adding took 14.217928171157837 s
Writing Index
Writing index took 0.3520667552947998 s
@ Completed
@ Stage 4
2022-08-29 09:15:07 | INFO | fairseq_cli.train | Namespace(activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='mbart_base', attention_dropout=0.0, batch_size=32, batch_size_valid=32, best_checkpoint_metric='bleu', bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, dstore_filename='/home/cluster/jgu/scratch/ssr/cli/out/mix/java/half_shared_datastore', dstore_fp16=True, dstore_size=2387231, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eval_bleu=True, eval_bleu_args='{"beam": 6}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, k=2, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, knn_lambda=0.5, knn_sim_metric='IP', knn_temperature=10.0, label_smoothing=0.1, langs='java,python,en_XX', layernorm_embedding=True, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format='json', log_interval=100, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, move_dstore_to_mem=True, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, only_train_final_output=False, optimizer='adam', optimizer_overrides='{}', partially_finetune=True, patience=5, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, prepend_bos=False, probe=32, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='/home/cluster/jgu/scratch/ssr/cli/out/mix/half_base_java_en_XX/checkpoint_best.pt', save_dir='/home/cluster/jgu/scratch/ssr/cli/out/mix/half_ada_shared_java_en_XX', save_interval=1, save_interval_updates=0, scoring='bleu', seed=42, sentence_avg=False, sentencepiece_model='/home/cluster/jgu/scratch/ssr/cli/sentencepiece/sentencepiece.bpe.model', shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='java', stop_time_hours=0, target_lang='en_XX', task='translation_from_pretrained_bart', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_gpu_to_search=True, use_knn_datastore=True, use_old_adam=False, user_dir='/home/cluster/jgu/scratch/ssr/cli', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=1000, weight_decay=0.0, zero_sharding='none')
2022-08-29 09:15:08 | INFO | fairseq.tasks.translation | [java] dictionary: 50001 types
2022-08-29 09:15:08 | INFO | fairseq.tasks.translation | [en_XX] dictionary: 50001 types
2022-08-29 09:15:08 | INFO | fairseq.data.data_utils | loaded 8714 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin/valid.java-en_XX.java
2022-08-29 09:15:08 | INFO | fairseq.data.data_utils | loaded 8714 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin/valid.java-en_XX.en_XX
2022-08-29 09:15:08 | INFO | fairseq.tasks.translation | /home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin valid java-en_XX 8714 examples
put index from cpu to gpu
Reading datastore took 1.4939312934875488 s
the datastore is /home/cluster/jgu/scratch/ssr/cli/out/mix/java/half_shared_datastore, size is 2387231, and dim is 768 
Loading to memory...
Loading to memory took 2.9018442630767822 s
2022-08-29 09:15:15 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (adaptive_block): AdaptiveBlock(
      (fc_unit): Linear(in_features=768, out_features=768, bias=False)
      (fc_unit2): Linear(in_features=768, out_features=768, bias=False)
      (fc_zero): Linear(in_features=768, out_features=768, bias=False)
      (fc_zero2): Linear(in_features=768, out_features=768, bias=False)
    )
    (output_projection): Linear(in_features=768, out_features=50005, bias=False)
  )
  (classification_heads): ModuleDict()
)
2022-08-29 09:15:15 | INFO | fairseq_cli.train | task: translation_from_pretrained_bart (TranslationFromPretrainedBARTTask)
2022-08-29 09:15:15 | INFO | fairseq_cli.train | model: mbart_base (BARTModel)
2022-08-29 09:15:15 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2022-08-29 09:15:15 | INFO | fairseq_cli.train | num. model params: 141580032 (num. trained: 2359296)
2022-08-29 09:15:15 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-08-29 09:15:15 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-08-29 09:15:15 | INFO | fairseq.trainer | detected shared parameter: decoder.adaptive_block.fc_unit.bias <- decoder.adaptive_block.fc_unit2.bias
2022-08-29 09:15:15 | INFO | fairseq.trainer | detected shared parameter: decoder.adaptive_block.fc_unit.bias <- decoder.adaptive_block.fc_zero.bias
2022-08-29 09:15:15 | INFO | fairseq.trainer | detected shared parameter: decoder.adaptive_block.fc_unit.bias <- decoder.adaptive_block.fc_zero2.bias
2022-08-29 09:15:15 | INFO | fairseq.trainer | detected shared parameter: decoder.adaptive_block.fc_unit.bias <- decoder.output_projection.bias
2022-08-29 09:15:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-08-29 09:15:15 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM2-32GB                    
2022-08-29 09:15:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-08-29 09:15:15 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-08-29 09:15:15 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 32
-----------------knn load part of model-----------------
2022-08-29 09:15:17 | INFO | fairseq.trainer | loaded checkpoint /home/cluster/jgu/scratch/ssr/cli/out/mix/half_base_java_en_XX/checkpoint_best.pt (epoch 22 @ 0 updates)
2022-08-29 09:15:17 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2022-08-29 09:15:17 | INFO | fairseq.trainer | loading train data for epoch 1
2022-08-29 09:15:17 | INFO | fairseq.data.data_utils | loaded 69708 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin/train.java-en_XX.java
2022-08-29 09:15:17 | INFO | fairseq.data.data_utils | loaded 69708 examples from: /home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin/train.java-en_XX.en_XX
2022-08-29 09:15:17 | INFO | fairseq.tasks.translation | /home/cluster/jgu/scratch/ssr/cli/out/mix/java/data-bin train java-en_XX 69708 examples
2022-08-29 09:15:17 | INFO | fairseq.trainer | begin training epoch 1
2022-08-29 09:15:47 | INFO | train_inner | {"epoch": 1, "update": 0.046, "loss": "3.253", "nll_loss": "1.285", "ppl": "2.44", "wps": "2172", "ups": "3.44", "wpb": "630.2", "bsz": "32", "num_updates": "100", "lr": "5e-06", "gnorm": "0.504", "train_wall": "29", "wall": "31"}
2022-08-29 09:16:18 | INFO | train_inner | {"epoch": 1, "update": 0.092, "loss": "3.206", "nll_loss": "1.256", "ppl": "2.39", "wps": "1962.4", "ups": "3.16", "wpb": "621.5", "bsz": "32", "num_updates": "200", "lr": "1e-05", "gnorm": "0.5", "train_wall": "32", "wall": "63"}
2022-08-29 09:16:50 | INFO | train_inner | {"epoch": 1, "update": 0.138, "loss": "3.191", "nll_loss": "1.244", "ppl": "2.37", "wps": "1970.2", "ups": "3.18", "wpb": "620", "bsz": "32", "num_updates": "300", "lr": "1.5e-05", "gnorm": "0.482", "train_wall": "31", "wall": "94"}
2022-08-29 09:17:21 | INFO | train_inner | {"epoch": 1, "update": 0.184, "loss": "3.216", "nll_loss": "1.276", "ppl": "2.42", "wps": "2174.9", "ups": "3.18", "wpb": "682.9", "bsz": "32", "num_updates": "400", "lr": "2e-05", "gnorm": "0.47", "train_wall": "31", "wall": "126"}
2022-08-29 09:17:58 | INFO | train_inner | {"epoch": 1, "update": 0.229, "loss": "3.226", "nll_loss": "1.288", "ppl": "2.44", "wps": "1828.9", "ups": "2.71", "wpb": "674.7", "bsz": "32", "num_updates": "500", "lr": "2.5e-05", "gnorm": "0.469", "train_wall": "37", "wall": "163"}
2022-08-29 09:18:31 | INFO | train_inner | {"epoch": 1, "update": 0.275, "loss": "3.179", "nll_loss": "1.24", "ppl": "2.36", "wps": "1996.2", "ups": "2.99", "wpb": "668.4", "bsz": "32", "num_updates": "600", "lr": "3e-05", "gnorm": "0.46", "train_wall": "33", "wall": "196"}
2022-08-29 09:19:06 | INFO | train_inner | {"epoch": 1, "update": 0.321, "loss": "3.165", "nll_loss": "1.224", "ppl": "2.34", "wps": "1857.3", "ups": "2.92", "wpb": "636.1", "bsz": "31.8", "num_updates": "700", "lr": "3.5e-05", "gnorm": "0.444", "train_wall": "34", "wall": "230"}
2022-08-29 09:19:37 | INFO | train_inner | {"epoch": 1, "update": 0.367, "loss": "3.167", "nll_loss": "1.229", "ppl": "2.34", "wps": "1966.6", "ups": "3.21", "wpb": "612.8", "bsz": "32", "num_updates": "800", "lr": "4e-05", "gnorm": "0.447", "train_wall": "31", "wall": "262"}
2022-08-29 09:20:08 | INFO | train_inner | {"epoch": 1, "update": 0.413, "loss": "3.202", "nll_loss": "1.27", "ppl": "2.41", "wps": "2177.2", "ups": "3.22", "wpb": "677.2", "bsz": "32", "num_updates": "900", "lr": "4.5e-05", "gnorm": "0.44", "train_wall": "31", "wall": "293"}
2022-08-29 09:20:44 | INFO | train_inner | {"epoch": 1, "update": 0.459, "loss": "3.236", "nll_loss": "1.311", "ppl": "2.48", "wps": "1966.4", "ups": "2.77", "wpb": "709.4", "bsz": "32", "num_updates": "1000", "lr": "5e-05", "gnorm": "0.425", "train_wall": "36", "wall": "329"}
2022-08-29 09:21:18 | INFO | train_inner | {"epoch": 1, "update": 0.505, "loss": "3.156", "nll_loss": "1.219", "ppl": "2.33", "wps": "1811.2", "ups": "2.97", "wpb": "610.7", "bsz": "32", "num_updates": "1100", "lr": "4.9995e-05", "gnorm": "0.434", "train_wall": "34", "wall": "363"}
2022-08-29 09:21:50 | INFO | train_inner | {"epoch": 1, "update": 0.551, "loss": "3.149", "nll_loss": "1.212", "ppl": "2.32", "wps": "1946.7", "ups": "3.12", "wpb": "623.3", "bsz": "32", "num_updates": "1200", "lr": "4.999e-05", "gnorm": "0.434", "train_wall": "32", "wall": "395"}
2022-08-29 09:22:24 | INFO | train_inner | {"epoch": 1, "update": 0.597, "loss": "3.118", "nll_loss": "1.181", "ppl": "2.27", "wps": "1832.4", "ups": "2.94", "wpb": "622.2", "bsz": "32", "num_updates": "1300", "lr": "4.9985e-05", "gnorm": "0.424", "train_wall": "34", "wall": "428"}
2022-08-29 09:22:56 | INFO | train_inner | {"epoch": 1, "update": 0.642, "loss": "3.163", "nll_loss": "1.228", "ppl": "2.34", "wps": "2011.8", "ups": "3.13", "wpb": "643.6", "bsz": "32", "num_updates": "1400", "lr": "4.998e-05", "gnorm": "0.426", "train_wall": "32", "wall": "460"}
2022-08-29 09:23:30 | INFO | train_inner | {"epoch": 1, "update": 0.688, "loss": "3.237", "nll_loss": "1.313", "ppl": "2.48", "wps": "2084.1", "ups": "2.95", "wpb": "705.6", "bsz": "32", "num_updates": "1500", "lr": "4.9975e-05", "gnorm": "0.436", "train_wall": "34", "wall": "494"}
2022-08-29 09:24:05 | INFO | train_inner | {"epoch": 1, "update": 0.734, "loss": "3.206", "nll_loss": "1.276", "ppl": "2.42", "wps": "1943.4", "ups": "2.81", "wpb": "691.8", "bsz": "32", "num_updates": "1600", "lr": "4.997e-05", "gnorm": "0.423", "train_wall": "35", "wall": "530"}
2022-08-29 09:24:38 | INFO | train_inner | {"epoch": 1, "update": 0.78, "loss": "3.225", "nll_loss": "1.3", "ppl": "2.46", "wps": "2074.1", "ups": "3.04", "wpb": "681.6", "bsz": "32", "num_updates": "1700", "lr": "4.9965e-05", "gnorm": "0.413", "train_wall": "33", "wall": "563"}
2022-08-29 09:25:12 | INFO | train_inner | {"epoch": 1, "update": 0.826, "loss": "3.159", "nll_loss": "1.226", "ppl": "2.34", "wps": "1940.6", "ups": "2.93", "wpb": "661.4", "bsz": "32", "num_updates": "1800", "lr": "4.996e-05", "gnorm": "0.409", "train_wall": "34", "wall": "597"}
2022-08-29 09:25:48 | INFO | train_inner | {"epoch": 1, "update": 0.872, "loss": "3.182", "nll_loss": "1.253", "ppl": "2.38", "wps": "2043.7", "ups": "2.79", "wpb": "732.5", "bsz": "32", "num_updates": "1900", "lr": "4.9955e-05", "gnorm": "0.41", "train_wall": "36", "wall": "633"}
2022-08-29 09:26:22 | INFO | train_inner | {"epoch": 1, "update": 0.918, "loss": "3.111", "nll_loss": "1.17", "ppl": "2.25", "wps": "1735.9", "ups": "2.92", "wpb": "594.6", "bsz": "32", "num_updates": "2000", "lr": "4.99499e-05", "gnorm": "0.413", "train_wall": "34", "wall": "667"}
2022-08-29 09:26:52 | INFO | train_inner | {"epoch": 1, "update": 0.964, "loss": "3.185", "nll_loss": "1.258", "ppl": "2.39", "wps": "2008.1", "ups": "3.36", "wpb": "597.1", "bsz": "32", "num_updates": "2100", "lr": "4.99449e-05", "gnorm": "0.412", "train_wall": "30", "wall": "697"}
2022-08-29 09:27:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
/net/cephfs/scratch/jgu/ssr/fairseq/utils.py:342: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2022-08-29 09:33:48 | INFO | valid | {"epoch": 1, "valid_loss": "5.909", "valid_nll_loss": "4.293", "valid_ppl": "19.6", "valid_bleu": "8.94", "valid_wps": "457.1", "valid_wpb": "652.8", "valid_bsz": "31.9", "valid_num_updates": "2179"}
2022-08-29 09:33:48 | INFO | fairseq_cli.train | begin save checkpoint
2022-08-29 09:33:50 | INFO | fairseq.checkpoint_utils | saved checkpoint /home/cluster/jgu/scratch/ssr/cli/out/mix/half_ada_shared_java_en_XX/checkpoint_best.pt (epoch 1 @ 2179 updates, score 8.94) (writing took 2.1947057880461216 seconds)
2022-08-29 09:33:50 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-08-29 09:33:50 | INFO | train | {"epoch": 1, "train_loss": "3.189", "train_nll_loss": "1.252", "train_ppl": "2.38", "train_wps": "1278.3", "train_ups": "1.96", "train_wpb": "653", "train_bsz": "32", "train_num_updates": "2179", "train_lr": "4.9941e-05", "train_gnorm": "0.441", "train_train_wall": "718", "train_wall": "1115"}
2022-08-29 09:33:51 | INFO | fairseq_cli.train | done training in 1113.8 seconds
/net/cephfs/scratch/jgu/ssr/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beams_buf = indices_buf // vocab_size
/net/cephfs/scratch/jgu/ssr/fairseq/sequence_generator.py:651: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  unfin_idx = idx // beam_size
@ Completed
@ Stage 5
/net/cephfs/scratch/jgu/ssr/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beams_buf = indices_buf // vocab_size
/net/cephfs/scratch/jgu/ssr/fairseq/sequence_generator.py:651: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  unfin_idx = idx // beam_size
WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.
c_bleu = 32.39 | s_bleu = 43.7 | meteor = 25.54 | rouge = 55.06
rouge_coco = 56.2
/net/cephfs/scratch/jgu/ssr/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beams_buf = indices_buf // vocab_size
/net/cephfs/scratch/jgu/ssr/fairseq/sequence_generator.py:651: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  unfin_idx = idx // beam_size
WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.
c_bleu = 33.96 | s_bleu = 44.32 | meteor = 26.19 | rouge = 55.43
rouge_coco = 56.49
@ Completed
